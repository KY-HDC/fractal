{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from model.resnet_v3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.mnist import *\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = data_normalize(train_ds).shuffle(buffer_size=10, seed=42).batch(batch_size).prefetch(1).take(10)\n",
    "test_ds = data_normalize(test_ds).shuffle(buffer_size=10, seed=42).batch(batch_size).prefetch(1).take(10)\n",
    "\n",
    "total_batch = train_ds.cardinality().numpy()\n",
    "total_tbatch = test_ds.cardinality().numpy()\n",
    "\n",
    "for batch in train_ds.as_numpy_iterator():\n",
    "    x = batch['image']\n",
    "    y = batch['label']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 10\n",
    "num_blocks = (3, 3, 3)\n",
    "c_hidden = (16, 32, 64)\n",
    "\n",
    "resnet = ResNet(num_classes=target_dim, act_fn=nn.relu, block_class=ResNetBlock, num_blocks=num_blocks, c_hidden=c_hidden)\n",
    "resnet20 = ResNet(10, nn.relu, ResNetBlock)\n",
    "variables = resnet20.init(jax.random.PRNGKey(1), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = variables['params']\n",
    "batch_stats = variables['batch_stats']\n",
    "\n",
    "y, updates = resnet20.apply(\n",
    "    {'params': params, 'batch_stats': batch_stats},\n",
    "    x,\n",
    "    on_train=True,\n",
    "    mutable=['batch_stats']\n",
    ")\n",
    "batch_stats = updates['batch_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "from typing import Any\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: Any\n",
    "\n",
    "state = TrainState.create(\n",
    "  apply_fn=resnet20.apply,\n",
    "  params=params,\n",
    "  batch_stats=batch_stats,\n",
    "  tx=optax.adam(1e-3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:25<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: TrainState, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits, updates = state.apply_fn(\n",
    "      {'params': params, 'batch_stats': state.batch_stats},\n",
    "      x=batch['image'], on_train=True, mutable=['batch_stats'])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits=logits, labels=batch['label']).mean()\n",
    "    return loss, (logits, updates)\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, (logits, updates)), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  state = state.replace(batch_stats=updates['batch_stats'])\n",
    "  metrics = {\n",
    "    'loss': loss,\n",
    "      'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label']),\n",
    "  }\n",
    "  return state, metrics\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "  for batch in train_ds.as_numpy_iterator():\n",
    "    state, metrics = train_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "px:   1%|          | 1/100 [00:50<1:22:52, 50.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m states, metrics \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m tqdm(lrs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     state, metric \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m     23\u001b[0m     metrics\u001b[38;5;241m.\u001b[39mappend(metric)\n",
      "Cell \u001b[1;32mIn[28], line 12\u001b[0m, in \u001b[0;36mtrain_custom\u001b[1;34m(lr, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mas_numpy_iterator():\n\u001b[1;32m---> 12\u001b[0m         state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, metrics\n",
      "File \u001b[1;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_custom(lr, epochs=10):\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=resnet20.apply,\n",
    "        params=params,\n",
    "        batch_stats=batch_stats,\n",
    "        tx=optax.adam(lr),\n",
    "        )\n",
    "    \n",
    "    for i in tqdm(range(epochs), leave=False, desc='epochs'):\n",
    "        for batch in train_ds.as_numpy_iterator():\n",
    "            state, metrics = train_step(state, batch)\n",
    "\n",
    "    return state, metrics\n",
    "\n",
    "\n",
    "lrs = jnp.linspace(1e-6, 1, 100)\n",
    "\n",
    "states, metrics = [], []\n",
    "for lr in tqdm(lrs, desc='px'):\n",
    "    state, metric = train_custom(lr, epochs=10)\n",
    "    states.append(state)\n",
    "    metrics.append(metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* jax+flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(variables, x: jnp.array, on_train=True):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    \n",
    "    # input.T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x, batch_stats['BatchNorm_0'] = batchnorm(x, params['BatchNorm_0'], batch_stats['BatchNorm_0'], on_train=on_train)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=on_train)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=on_train)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "    \n",
    "    def mode_train():\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        r_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        r_var = momentum * running_var + (1 - momentum) * var\n",
    "        return (x - mu) / jnp.sqrt(var + eps), r_mu, r_var\n",
    "    \n",
    "    def mode_inference():\n",
    "        r_mu = running_mu\n",
    "        r_var = running_var\n",
    "        return (x - r_mu) / jnp.sqrt(r_var + eps), r_mu, r_var\n",
    "        \n",
    "    x, running_mu, running_var = jax.lax.cond(on_train, mode_train, mode_inference)\n",
    "    \n",
    "    x = gamma * x + beta\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=3)\n",
    "def loss_fn(variables, x, y, on_train=True):\n",
    "    logits, variables = net(variables, x, on_train=on_train)\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-10, 1.), y).mean(), (logits, variables)\n",
    "\n",
    "@partial(jax.vmap, in_axes=(0, None, None, 0))\n",
    "def update_fn(variables, x, y, lr):\n",
    "    (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (loss, logits)\n",
    "\n",
    "def train_custom3(lr, epochs=10):\n",
    "    \n",
    "    for i in tqdm(range(epochs), leave=False, desc='epochs'):\n",
    "        for batch in train_ds.as_numpy_iterator():\n",
    "            state, metrics = train_step(state, batch)\n",
    "\n",
    "    return state, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATTNtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
