{"cells":[{"cell_type":"markdown","metadata":{"id":"BVEaikRONL9p"},"source":["# Loading libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50527,"status":"ok","timestamp":1711064890777,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"IfZdetlbL5AC","outputId":"d9ffed76-6bde-49e6-ae31-e473c452287e"},"outputs":[],"source":["# !pip install porespy\n","# !pip install pypardiso"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711066099090,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"L7Ev2fyd0MlE"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-24 14:59:34.910599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import os\n","# os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=4'\n","os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,4' \n","# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n","# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".XX\"\n","# os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n","\n","import jax\n","from jax import lax, random, config, numpy as jnp\n","config.update('jax_enable_x64', True)\n","from jax.sharding import Mesh, PartitionSpec, NamedSharding\n","from jax.experimental import mesh_utils\n","from jax.lax import with_sharding_constraint\n","\n","import flax\n","from flax import struct, traverse_util, linen as nn\n","from flax.core import freeze, unfreeze\n","from flax.training import train_state, checkpoints, orbax_utils\n","from flax.training.train_state import TrainState\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.animation as animation\n","from matplotlib import cm\n","\n","import optax\n","import orbax\n","import porespy as ps\n","import numpy as np\n","import pickle\n","import shutil\n","import time\n","import datetime\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from functools import partial\n","from scipy import ndimage\n","from typing import Callable, Any\n","from pprint import pprint\n","from tqdm import tqdm\n","\n","import telegram\n","import asyncio\n","from datetime import datetime\n","from tqdm.notebook import trange, tqdm\n","\n","\n","TOKEN = '6740952693:AAFOUwNFVu2O3Bpf7nlKwIlDzyNaarN7Fl8'\n","CHAT_ID = '5110804803'\n","\n","bot = telegram.Bot(TOKEN)"]},{"cell_type":"markdown","metadata":{"id":"OGuOPgSHPGV5"},"source":["# Hyperparams"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711067420697,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"-7LcZrOiPIAw"},"outputs":[],"source":["# Model's hyperparams\n","num_epochs = 10 # 50\n","batch_size = 100\n","nonlinearity = nn.relu\n","lrbatch = 2500     # lrband 9500~10000 -> BooM!\n","\n","# Plotting's hyperparams\n","mnmx = [-3, 6, -3, 6]\n","resolution = 128   # 1024\n","figsize = (8, 8)\n","dpi = 100"]},{"cell_type":"markdown","metadata":{"id":"t_zQ0IGOONCZ"},"source":["# Dataset loading"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":683,"status":"ok","timestamp":1711066593812,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"Eeqjs21VOXdi"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-24 14:59:47.451083: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n"]}],"source":["train_ds = tfds.load('mnist', split='train')\n","test_ds = tfds.load('mnist', split='test')\n","\n","def data_normalize(ds):\n","    return ds.map(lambda sample: {\n","        'image': tf.cast(sample['image'], tf.float32) / 256.,\n","        'label': sample['label']\n","    })\n","\n","train_ds = data_normalize(train_ds)\n","test_ds = data_normalize(test_ds)\n","\n","# train_ds = train_ds.repeat(num_epochs).shuffle(1024).batch(batch_size).prefetch(1)\n","train_ds = train_ds.shuffle(1024).batch(batch_size).prefetch(1)\n","test_ds = test_ds.shuffle(1024).batch(batch_size).prefetch(1)\n","\n","\n","# train_ds = train_ds.repeat(100).shuffle(1024).batch(32).prefetch(1).take(1000)"]},{"cell_type":"markdown","metadata":{"id":"_LmHzobLPZwR"},"source":["# Define the Sharding"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1711066597188,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"A7YdUpWLPD8T"},"outputs":[],"source":["P = PartitionSpec\n","mesh = Mesh(mesh_utils.create_device_mesh((2, 2)), axis_names=('x', 'y'))\n","\n","def with_mesh(f):\n","    def wrapper(*args, **kwargs):\n","        with mesh:\n","            return f(*args, **kwargs)\n","    return wrapper"]},{"cell_type":"markdown","metadata":{"id":"bomvFDHvQFF4"},"source":["# Build the model"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1711066599607,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"drfPrOZL5ohT"},"outputs":[],"source":["# The grid of learning_rates\n","mn1, mx1, mn2, mx2 = mnmx\n","gg1 = jnp.logspace(mn1, mx1, resolution)\n","gg2 = jnp.logspace(mn2, mx2, resolution)\n","lr0, lr1 = jnp.meshgrid(gg2, gg1)\n","lrband = jnp.stack([lr0.ravel(), lr1.ravel()], axis=-1)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711066600722,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"ylsWh7duPvxT","outputId":"db157020-ffa0-4209-a832-06fed8f44724"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'params': {'Dense_0': {'kernel': (784, 784)},\n","            'Dense_1': {'kernel': (784, 10)}}}\n"]}],"source":["class DNN(nn.Module):\n","\n","    width = 28*28\n","    use_bias = True\n","    act_fn: Callable\n","\n","    @nn.compact\n","    def __call__(self, x):\n","        # print(x.shape)\n","        x = x.reshape((x.shape[0], -1))     # Normal mode\n","        # x = x.reshape((x.shape[0] * x.shape[1], ))     # Vmap mode\n","\n","        x = nn.Dense(self.width, use_bias=False)(x)\n","        x = self.act_fn(x)\n","        x = nn.Dense(10, use_bias=False)(x)\n","        x = self.act_fn(x)\n","        # x = nn.BatchNorm(use_running_average=not train)(x)\n","        return x\n","        # return x / jnp.sqrt(self.width)\n","\n","\n","# 'act_fn' can be 'nn.relu'(ReLU), 'nn.activation.tanh'(tanh) or 'lambda x: x'(identity).\n","dnn = DNN(act_fn=nonlinearity)\n","\n","x = jnp.zeros((batch_size, 28, 28, 1))\n","rng = jax.random.PRNGKey(42)\n","# rng = jax.random.split(rng, 2)\n","# variables = dnn.init(rng, x, train=False)\n","variables = dnn.init(rng, x)\n","pprint(jax.tree_map(jnp.shape, variables))\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":458,"status":"ok","timestamp":1711067402465,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"REtObdjBiTBd"},"outputs":[],"source":["# Just one img\n","@jax.jit\n","def train_step(state, lrband, batch):\n","\n","    '''MiniBatch are needed soon.'''\n","    # 'Phase_space' will fixed as 'param_vs_lr'.\n","    state = state.replace(params={\n","        'Dense_0': {'kernel': jnp.array(state.params['Dense_0']['kernel']) + jnp.array(lrband[0])},\n","        'Dense_1': {'kernel': state.params['Dense_1']['kernel']}\n","    })  # new theta\n","\n","    @jax.jit\n","    def loss_fn(params):\n","\n","        logits = state.apply_fn(\n","            {'params': state.params},\n","            x=batch['image']\n","        )\n","\n","        loss = optax.softmax_cross_entropy_with_integer_labels(\n","            logits=logits, labels=batch['label']\n","        ).mean()    # mean for 10-label\n","\n","        return loss, logits\n","    \n","    grad_fn = jax.value_and_grad(loss_fn, has_aux=True, allow_int=True)\n","    (loss, logits), grads = grad_fn(state.params)\n","    state = state.apply_gradients(grads=grads)\n","    # metrics = {\n","    #     'loss': loss.mean(),\n","    #     'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label'])\n","    # }\n","    \n","    # loss = jnp.asarray(loss)\n","    return state, loss.mean()\n","\n","@jax.jit\n","def eval_step(state, batch):\n","    logits = state.apply_fn(\n","        {'params': state.params, 'batch_stats': state.batch_stats},\n","        x=batch['image'],\n","        train=False\n","    )\n","    loss = optax.softmax_cross_entropy_with_integer_labels(\n","        logits=logits, labels=batch['label']\n","    ).mean()\n","    return state, loss.mean()\n","\n","@partial(jax.jit, static_argnums=(0,))\n","@partial(jax.vmap, in_axes=(None, 0))\n","def ready_to_train(num_epochs, lrband):\n","\n","    # Initialize\n","    variables = dnn.init(jax.random.PRNGKey(int(time.time())), jnp.ones((batch_size, 28, 28, 1)))\n","    state = TrainState.create(      # The states are vmapped by lrband.\n","        apply_fn=dnn.apply,\n","        params=variables['params'],\n","        tx=optax.sgd(lrband[1])     # learning rates, applied 'hparams_f'\n","        )\n","\n","    # Training session\n","    loss_archive = []\n","    # for epoch in trange(num_epochs, desc='Epochs'):\n","    #     loss_batch = 0.\n","    #     for batch in tqdm(train_ds.as_numpy_iterator(), desc='Iters', total=train_ds.cardinality().numpy()):\n","    #         state, loss = train_step(state, lrband, batch)\n","    #         loss_batch += loss\n","    #     loss_batch /= train_ds.cardinality().numpy()\n","    #     loss_archive.append(loss_batch)\n","    for epoch in range(num_epochs):\n","        loss_batch = 0.\n","        for batch in train_ds.as_numpy_iterator():\n","            state, loss = train_step(state, lrband, batch)\n","            loss_batch += loss\n","        loss_batch /= train_ds.cardinality().numpy()\n","        loss_archive.append(loss_batch)\n","        \n","    # Test session\n","    testloss_archive = []\n","    \n","\n","    # return convergence_measure_vmap(jnp.stack(loss_archive, axis=-1))\n","    return jnp.array(loss_archive)\n","\n","\n","@jax.jit\n","@partial(jax.vmap, in_axes=(0,), out_axes=0)\n","def convergence_measure_vmap(v, max_val=1e6):\n","    '''v is loss.'''\n","\n","    fin = jnp.isfinite(v)\n","    v = v*fin + max_val*(1-fin)\n","\n","    v /= v[0]\n","    exceeds = (v > max_val)\n","    v = v*(1-exceeds) + max_val*exceeds\n","\n","    converged = (jnp.mean(v[-20:]) < 1)\n","    return jnp.where(converged, -jnp.sum(v), jnp.sum(1/v))\n","\n","\n","def train(num_epochs, lrband, lrbatch=100):\n","    bs = lrband.shape[0]\n","\n","    if bs > lrbatch:\n","        print(f\"[WARNING!!] The band of learning rate is too long. It will be cut. Current length: {bs} -> {bs//2}\")\n","        return jnp.concatenate(\n","            (jnp.array(train(num_epochs, lrband[:bs//2], lrbatch)),\n","            jnp.array(train(num_epochs, lrband[bs//2:], lrbatch))),\n","        axis=0)\n","    return ready_to_train(num_epochs, lrband)\n","\n","\n","def gen_img(saveas=None):\n","    losses = train(num_epochs, lrband, lrbatch)\n","    V = convergence_measure_vmap(jnp.stack(losses, axis=0))\n","    V = V.reshape((resolution, resolution))\n","    if saveas != None:\n","        jnp.save(saveas, V)\n","    return V"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 16384 -> 8192\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 8192 -> 4096\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 4096 -> 2048\n"]},{"name":"stderr","output_type":"stream","text":["2024-03-24 19:45:57.066085: E external/xla/xla/service/slow_operation_alarm.cc:65] \n","********************************\n","[Compiling module jit_ready_to_train] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n","********************************\n"]}],"source":["# losses = train(num_epochs, lrband, lrbatch)\n","# V = convergence_measure_vmap(jnp.stack(losses, axis=-1))\n","# V.reshape((resolution, resolution))\n","\n","\n","# train_step --> no problem\n","# variables = dnn.init(jax.random.PRNGKey(int(time.time())), jnp.ones((1, 28, 28, 1)))\n","# state = TrainState.create(      # The states are vmapped by lrband.\n","#     apply_fn=dnn.apply,\n","#     params=variables['params'],\n","#     tx=optax.sgd(lrband[0, 1])     # learning rates, applied 'hparams_f'\n","#     )\n","\n","# for batch in train_ds.as_numpy_iterator():\n","#     break\n","# state, loss1 = train_step(state, lrband[0, :], batch)\n","# state, loss2 = train_step(state, lrband[0, :], batch)\n","\n","# print((loss1 + loss2) / 2)\n","\n","# ready_to_train --> no problem, but need to shrink 'lrbatch' threshold\n","# v = ready_to_train(2, lrband[:4, :])\n","# jnp.stack(v, axis=-1)\n","\n","# train --> lrbatch: 2500 is safe! But over 5000, BooM!\n","# lrbatch = 2500\n","# v = train(2, lrband, lrbatch=lrbatch)\n","\n","recoding_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","saveas = f'./output/npy/lossmap_DNN_epoch{num_epochs}_batch{batch_size}_actFN{nonlinearity.__name__}_resolution{resolution}_dpi{dpi}_time.npy'\n","V = gen_img(saveas=saveas)\n","\n","from datetime import datetime\n","recoding_time = datetime.now().strftime(\"%Y.%m.%d %H:%M:%S\")\n","msg = f\"[{recoding_time}]\\nLossmap was generated and saved.\\nSaved as: {saveas}\"\n","await bot.sendMessage(chat_id=CHAT_ID, text=msg)"]},{"cell_type":"markdown","metadata":{"id":"jVXsiZg7fWKo"},"source":["## Calculate the Fractal Dimension"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1711066642517,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"MTFWkKfHfDi-"},"outputs":[],"source":["def extract_edges(X):\n","    \"\"\"\n","    define edges as sign changes in the scalar representing convergence or\n","    divergence rate -- on one side of the edge training converges,\n","    while on the other side of the edge training diverges\n","    \"\"\"\n","\n","    Y = jnp.stack((X[1:,1:], X[:-1,1:], X[1:,:-1], X[:-1,:-1]), axis=-1)\n","    Z = jnp.sign(jnp.max(Y, axis=-1)*jnp.min(Y, axis=-1))\n","    return Z<0\n","\n","def estimate_fractal_dimension(hist_video, show_plot=True):\n","    edges = [extract_edges(U[0]) for U in hist_video]\n","    box_counts = [ps.metrics.boxcount(U) for U in edges]\n","    all_images = np.concatenate([bc.slope for bc in box_counts])\n","\n","    if show_plot:\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n","        ax1.set_yscale('log')\n","        ax1.set_xscale('log')\n","        ax1.set_xlabel('box edge length')\n","        ax1.set_ylabel('number of boxes spanning phases')\n","        ax2.set_xlabel('box edge length')\n","        ax2.set_ylabel('image')\n","        ax2.set_xscale('log')\n","\n","        for bc in box_counts:\n","            ax1.plot(bc.size, bc.count,'-o')\n","            ax2.plot(bc.size, bc.slope,'-o');\n","\n","    mfd = np.median(all_images)\n","    print(f'median fractal dimension estimate {mfd}')\n","\n","    return mfd"]},{"cell_type":"markdown","metadata":{"id":"H3b0AkZafX-7"},"source":["## Image restretch"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1711066646481,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"2pjP2dTlfT0s"},"outputs":[],"source":["def cdf_img(x, x_ref, buffer=0.25):\n","    \"\"\"\n","    rescale x, relative to x_ref (x_ref is often the same as x), to achieve a uniform\n","    distribution over values with positive and negative intensities, but also to\n","    preserve the sign of x. This makes for a visualization that shows more\n","    structure.\n","    \"\"\"\n","\n","    u = jnp.sort(x_ref.ravel())\n","\n","    num_neg = jnp.sum(u<0)\n","    num_nonneg = u.shape[0] - num_neg\n","    v = jnp.concatenate((jnp.linspace(-1,-buffer,num_neg), jnp.linspace(buffer,1,num_nonneg)), axis=0)\n","\n","    y = jnp.interp(x, u, v)\n","    return -y"]},{"cell_type":"markdown","metadata":{"id":"QqIaXZjOfnOa"},"source":["# Interactive Figure"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711066667531,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"UMCJMBj2fjxl"},"outputs":[],"source":["def truncate_sci_notation(numbers):\n","    \"\"\"\n","    keeping enough significant digits that the\n","    numbers disagree in four digits\n","    \"\"\"\n","\n","    # Convert numbers to scientific notation\n","    n1_sci, n2_sci = \"{:.15e}\".format(numbers[0]), \"{:.15e}\".format(numbers[1])\n","\n","    # Extract the significant parts and exponents\n","    sig_n1, exp_n1 = n1_sci.split('e')\n","    sig_n2, exp_n2 = n2_sci.split('e')\n","\n","    # Find the first position at which they disagree\n","    min_len = min(len(sig_n1), len(sig_n2))\n","    truncate_index = min_len\n","\n","    for i in range(min_len):\n","        if (sig_n1[i] != sig_n2[i]) or (exp_n1 != exp_n2):\n","            # +4 accounts for 4 digits after the first disagreement\n","            truncate_index = i + 4\n","            if i == 0:\n","                truncate_index += 1 # Account for decimal point\n","            break\n","\n","    exp_n1 = exp_n1[0] + exp_n1[2]\n","    exp_n2 = exp_n2[0] + exp_n2[2]\n","    if (exp_n1 == \"+00\") and (exp_n2 == \"+00\"):\n","        # don't bother with scientific notation if exponent is 0\n","        return [sig_n1[:truncate_index], sig_n2[:truncate_index]]\n","\n","    # Truncate and reconstruct the scientific notation\n","    truncated_n1 = \"{}e{}\".format(sig_n1[:truncate_index], exp_n1)\n","    truncated_n2 = \"{}e{}\".format(sig_n2[:truncate_index], exp_n2)\n","\n","    return [truncated_n1, truncated_n2]\n","\n","def tickslabels(mnmx):\n","    return mnmx, truncate_sci_notation(10.**np.array(mnmx))"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1711066860909,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"16aLgdKDfzsk"},"outputs":[],"source":["cids = []\n","click_event = [None]\n","\n","def onclick(event):\n","    click_event[0] = (event.xdata, event.ydata)\n","\n","def onrelease(event, fig, im, rect, mnmx, img, recalculate_image=True):\n","    if click_event[0] is None:\n","        return\n","\n","    e0 = [click_event[0][0], event.xdata]\n","    e1 = [click_event[0][1], event.ydata]\n","\n","    for v in e0+e1:\n","        if v is None:\n","            return\n","\n","    newmnmx = [np.min(e1), np.max(e1), np.min(e0), np.max(e0)]\n","\n","    min_w = (mnmx[1] - mnmx[0])/20\n","    if newmnmx[1] - newmnmx[0] < min_w:\n","        c = (newmnmx[1] + newmnmx[0])/2.\n","        newmnmx[0] = c - min_w/2\n","        newmnmx[1] = c + min_w/2\n","    min_w = (mnmx[3] - mnmx[2])/20\n","    if newmnmx[1] - newmnmx[0] < min_w:\n","        c = (newmnmx[3] + newmnmx[2])/2.\n","        newmnmx[2] = c - min_w/2\n","        newmnmx[3] = c + min_w/2\n","\n","    for v in newmnmx:\n","        if v is None:\n","            return\n","    plot_img(img, mnmx, newmnmx, fig=fig, im=im, rect=rect)\n","    plt.draw()\n","\n","    if recalculate_image:\n","        click_event[0] = None\n","        mnmx = newmnmx\n","        img = gen_img(mnmx)\n","        plot_img(img, mnmx, None, fig=fig, im=im, rect=rect)\n","\n","def plot_img(image, mnmx, newmnmx=None, fig=None, im=None, rect=None,\n","            handler=True, savename=None,\n","            reference_scale=None,\n","            cmap='Spectral',\n","            title=\"\"\n","            ):\n","    mn1, mx1, mn2, mx2 = mnmx\n","\n","    if reference_scale is None:\n","        reference_scale = image\n","\n","    image = cdf_img(image, reference_scale)\n","\n","    ax1 = None\n","    if fig is None:\n","        fig, (ax1) = plt.subplots(figsize=figsize, dpi=dpi)\n","        im = ax1.imshow(image,\n","                    extent=[mn2, mx2, mn1, mx1],\n","                    origin='lower',\n","                    vmin=-1, vmax=1,\n","                    cmap=cmap,\n","                    aspect='auto',\n","                    interpolation='nearest'\n","                    )\n","    if title is None:\n","        title = f'Trainability dependence on parameter initialization and learning rate \\n1 hidden layer, {nonlinearity}, batch size={batch_size}'\n","    if not title == \"\":\n","        plt.title(title)\n","    ax1.set_ylabel('Learning rate')\n","    ax1.set_xlabel('Input layer weight offset')\n","\n","    rect = patches.Rectangle((mn2, mn1), mx2-mn2, mx1-mn1, linewidth=1, edgecolor='r', facecolor='none')\n","    ax1.add_patch(rect)\n","\n","    im.set_extent([mn2, mx2, mn1, mx1])\n","    im.set_data(image)\n","\n","    # Set the new tick positions on the x-axis\n","    aaxx = plt.gca()\n","    aaxx.set_xticks(*tickslabels([mn2, mx2]))\n","    aaxx.set_yticks(*tickslabels([mn1, mx1]), rotation=90)\n","\n","    labels = aaxx.get_xticklabels()\n","    labels[0].set_horizontalalignment('left')\n","    labels[1].set_horizontalalignment('right')\n","    labels = aaxx.get_yticklabels()\n","    labels[0].set_verticalalignment('bottom')\n","    labels[1].set_verticalalignment('top')\n","\n","    if handler and (newmnmx is None):\n","        image_history.append((image, mnmx))\n","\n","    if newmnmx:\n","        mn1, mx1, mn2, mx2 = newmnmx\n","    rect.set_xy((mn2, mn1))\n","    rect.set_width(mx2-mn2)\n","    rect.set_height(mx1-mn1)\n","\n","    if handler:\n","        while len(cids) > 0:\n","            fig.canvas.mpl_disconnect(cids.pop())\n","\n","        def onrelease_partial(event):\n","            return onrelease(event, fig, im, rect, mnmx, img)\n","        def onmotion_partial(event):\n","            return onrelease(event, fig, im, rect, mnmx, img, recalculate_image=False)\n","\n","        cids.append(fig.canvas.mpl_connect('button_press_event', onclick))\n","        cids.append(fig.canvas.mpl_connect('button_release_event', onrelease_partial))\n","        # cids.append(fig.canvas.mpl_connect('motion_notify_event', onmotion_partial))\n","\n","    plt.tight_layout()\n","\n","    plt.draw()\n","\n","    if savename:\n","        plt.savefig(savename)\n","\n","    return fig, ax1, im"]},{"cell_type":"markdown","metadata":{"id":"2y4pvvSeiJ6l"},"source":["# Fractal Zoom"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1711066944057,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"K4SLTo2ZiGx0"},"outputs":[],"source":["def zoom_out_sequence(hist_final, growth_factor=2., max_scale=6):\n","    \"\"\"\n","    generate a sequence of (image, bounds) zooming out from the (image, bounds) in hist_final\n","    \"\"\"\n","\n","    image, mnmx = hist_final\n","\n","    cT = np.array([(mnmx[0] + mnmx[1])/2., (mnmx[2] + mnmx[3])/2.])\n","    wT = np.array([mnmx[1] - mnmx[0], mnmx[3] - mnmx[2]])\n","\n","    hist = [(image, mnmx)]\n","    w_scale = 1.\n","    while np.min(wT * w_scale) < max_scale:\n","        w_scale *= 2\n","        mnmx = [\n","            cT[0] - w_scale * wT[0]/2.,\n","            cT[0] + w_scale * wT[0]/2.,\n","            cT[1] - w_scale * wT[1]/2.,\n","            cT[1] + w_scale * wT[1]/2.,\n","        ]\n","        hist.insert(0, (np.zeros((2,2)), mnmx))\n","\n","    return hist\n","\n","def increase_resolution(history, target_res):\n","    \"\"\"\n","    Increase the resolution of images of a fractal landscape that we've already\n","    generated.\n","\n","    Find the first entry in history with resolution below target_res, and increase\n","    its resolution. If all images are already at least the target resolution,\n","    return False.\n","    \"\"\"\n","\n","    new_h = []\n","    for ii in range(len(history)):\n","        h = history[ii]\n","        image, mnmx = h\n","        if image.shape[0] < target_res:\n","            current_time = datetime.datetime.now()\n","            print( f\"increasing resolution of {ii} / {len(history)} at {current_time}, current resolution is {image.shape}\")\n","            image = gen_img(mnmx, resolution=target_res)\n","            history[ii] = (image, mnmx)\n","            return True\n","    return False\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":287,"status":"ok","timestamp":1711067052467,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"eqih8tKBicLO"},"outputs":[],"source":["def interpolate_history(hist1, hist2, alpha):\n","    \"\"\"\n","    get the mnmx (hyperparameter bounding box) value for a fraction alpha between\n","    two images\n","    \"\"\"\n","\n","    _, mnmx1 = hist1\n","    _, mnmx2 = hist2\n","\n","    if alpha == 0:\n","        # avoid NaNs on very last frame\n","        return mnmx1\n","\n","    w1 = np.array([mnmx1[1] - mnmx1[0], mnmx1[3] - mnmx1[2]])\n","    w2 = np.array([mnmx2[1] - mnmx2[0], mnmx2[3] - mnmx2[2]])\n","    c1 = np.array([(mnmx1[0] + mnmx1[1])/2, (mnmx1[2] + mnmx1[3])/2])\n","    c2 = np.array([(mnmx2[0] + mnmx2[1])/2, (mnmx2[2] + mnmx2[3])/2])\n","\n","    gamma = np.exp((1-alpha)*0 + alpha*np.log(w2/w1))\n","\n","    # ct = cstar + (c1 - cstar)*gamma\n","    # c1 = cstar + (c1 - cstar)*1\n","    # c2 = cstar + (c1 - cstar)*w2/w1\n","    cstar = (c2 - c1*w2/w1) / (1 - w2 / w1)\n","\n","    ct = cstar + (c1 - cstar)*gamma\n","    hwt = gamma*w1\n","\n","    return [ct[0] - hwt[0]/2, ct[0] + hwt[0]/2, ct[1] - hwt[1]/2, ct[1] + hwt[1]/2]\n","\n","\n","def em(extent_rev):\n","    return [extent_rev[2], extent_rev[3], extent_rev[0], extent_rev[1]]\n","\n","def make_animator(history, timesteps_per_transition=60, reference_scale=None, cmap='Spectral'):\n","\n","    fig, ax, im1 = plot_img(history[0][0], history[0][1], newmnmx=None,\n","                            handler=False, reference_scale=reference_scale, cmap=cmap)\n","\n","    im2 = ax.imshow(\n","        jnp.zeros_like(history[1][0]), extent=em(history[1][1]), origin='lower',\n","        vmin = -1, vmax = 1,\n","        cmap=cmap,\n","        aspect='auto',\n","        interpolation='nearest'\n","        )\n","\n","    im3 = ax.imshow(\n","        jnp.zeros_like(history[1][0]), extent=em(history[1][1]), origin='lower',\n","        vmin = -1, vmax = 1,\n","        cmap=cmap,\n","        aspect='auto',\n","        interpolation='nearest'\n","        )\n","\n","    def animate(n):\n","        hist_index = n // timesteps_per_transition\n","        alpha = (n % timesteps_per_transition) / timesteps_per_transition\n","\n","        hist1 = history[hist_index]\n","        if hist_index >= len(history)-1:\n","            hist2 = hist1 # very last frame\n","        else:\n","            hist2 = history[hist_index+1]\n","        if hist_index >= len(history)-2:\n","            hist3 = hist2 # very last frame\n","        else:\n","            hist3 = history[hist_index+2]\n","\n","        lims = interpolate_history(hist1, hist2, alpha)\n","\n","        # interpolation scheme for image restretch / colormap\n","        alpha_area = jnp.sin(alpha*np.pi/2)**2\n","\n","        print(f'frame {n} / {timesteps_per_transition*len(history)}, zoom step {hist_index} / {len(history)}', end='\\r', flush=True)\n","\n","        img_1 = (1-alpha_area)*cdf_img(hist1[0], hist1[0]) + alpha_area*cdf_img(hist1[0], hist2[0])\n","        img_2 = (1-alpha_area)*cdf_img(hist2[0], hist1[0]) + alpha_area*cdf_img(hist2[0], hist2[0])\n","        img_3 = (1-alpha_area)*cdf_img(hist3[0], hist1[0]) + alpha_area*cdf_img(hist3[0], hist2[0])\n","\n","        im1.set_data(img_1)\n","        im1.set_extent(em(hist1[1]))\n","        im2.set_data(img_2)\n","        im2.set_extent(em(hist2[1]))\n","        im3.set_data(img_3)\n","        im3.set_extent(em(hist3[1]))\n","        im3.set_alpha(alpha)\n","\n","        ax.set_ylim(lims[0], lims[1])\n","        ax.set_xlim(lims[2], lims[3])\n","\n","        # Set the new tick positions\n","        ax.set_xticks(*tickslabels([lims[2], lims[3]]))\n","        ax.set_yticks(*tickslabels([lims[0], lims[1]]), rotation=90)\n","\n","        labels = ax.get_xticklabels()\n","        labels[0].set_horizontalalignment('left')\n","        labels[1].set_horizontalalignment('right')\n","        labels = ax.get_yticklabels()\n","        labels[0].set_verticalalignment('bottom')\n","        labels[1].set_verticalalignment('top')\n","\n","        return fig,\n","\n","    anim = animation.FuncAnimation(fig,animate,frames=timesteps_per_transition*(len(history)-1)+1, repeat=False)\n","    return anim"]},{"cell_type":"markdown","metadata":{"id":"Hu8ETD8Ci0nM"},"source":["# Generate the Images & Movies"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1711067475638,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"fDRYz6j0kZR9"},"outputs":[],"source":["image_history = []"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"7D-sZimXi6lU"},"outputs":[{"name":"stdout","output_type":"stream","text":["[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 16777216 -> 8388608\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 8388608 -> 4194304\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 4194304 -> 2097152\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 2097152 -> 1048576\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 1048576 -> 524288\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 524288 -> 262144\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 262144 -> 131072\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 131072 -> 65536\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 65536 -> 32768\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 32768 -> 16384\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 16384 -> 8192\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 8192 -> 4096\n","[WARNING!!] The band of learning rate is too long. It will be cut. Current length: 4096 -> 2048\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 12/1000 [10:06<13:52:23, 50.55s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mion()\n\u001b[0;32m----> 4\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mgen_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m plot_img(img, mnmx, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n","Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mgen_img\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_img\u001b[39m():\n\u001b[0;32m---> 91\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     V \u001b[38;5;241m=\u001b[39m convergence_measure_vmap(jnp\u001b[38;5;241m.\u001b[39mstack(losses, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m V\u001b[38;5;241m.\u001b[39mreshape((resolution, resolution))\n","Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, lrband, lrbatch)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bs \u001b[38;5;241m>\u001b[39m lrbatch:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARNING!!] The band of learning rate is too long. It will be cut. Current length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m---> 84\u001b[0m         (jnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrband\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrbatch\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     85\u001b[0m         jnp\u001b[38;5;241m.\u001b[39marray(train(num_epochs, lrband[bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:], lrbatch))),\n\u001b[1;32m     86\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_to_train(num_epochs, lrband)\n","Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, lrband, lrbatch)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bs \u001b[38;5;241m>\u001b[39m lrbatch:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARNING!!] The band of learning rate is too long. It will be cut. Current length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m---> 84\u001b[0m         (jnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrband\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrbatch\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     85\u001b[0m         jnp\u001b[38;5;241m.\u001b[39marray(train(num_epochs, lrband[bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:], lrbatch))),\n\u001b[1;32m     86\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_to_train(num_epochs, lrband)\n","    \u001b[0;31m[... skipping similar frames: train at line 84 (10 times)]\u001b[0m\n","Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, lrband, lrbatch)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bs \u001b[38;5;241m>\u001b[39m lrbatch:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARNING!!] The band of learning rate is too long. It will be cut. Current length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m---> 84\u001b[0m         (jnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrband\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrbatch\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     85\u001b[0m         jnp\u001b[38;5;241m.\u001b[39marray(train(num_epochs, lrband[bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:], lrbatch))),\n\u001b[1;32m     86\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_to_train(num_epochs, lrband)\n","Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, lrband, lrbatch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARNING!!] The band of learning rate is too long. It will be cut. Current length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m     84\u001b[0m         (jnp\u001b[38;5;241m.\u001b[39marray(train(num_epochs, lrband[:bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m], lrbatch)),\n\u001b[1;32m     85\u001b[0m         jnp\u001b[38;5;241m.\u001b[39marray(train(num_epochs, lrband[bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:], lrbatch))),\n\u001b[1;32m     86\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mready_to_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrband\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/api.py:1240\u001b[0m, in \u001b[0;36mvmap.<locals>.vmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m in_axes_flat \u001b[38;5;241m=\u001b[39m flatten_axes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap in_axes\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_tree, (in_axes, \u001b[38;5;241m0\u001b[39m), kws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1238\u001b[0m axis_size_ \u001b[38;5;241m=\u001b[39m (axis_size \u001b[38;5;28;01mif\u001b[39;00m axis_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m               _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1240\u001b[0m out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mbatching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvmap out_axes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspmd_axis_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspmd_axis_name\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree(), out_flat)\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/linear_util.py:188\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    192\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    193\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n","Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mready_to_train\u001b[0;34m(num_epochs, lrband)\u001b[0m\n\u001b[1;32m     51\u001b[0m loss_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mas_numpy_iterator():\n\u001b[0;32m---> 53\u001b[0m     state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     loss_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     55\u001b[0m loss_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mcardinality()\u001b[38;5;241m.\u001b[39mnumpy()\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/pjit.py:250\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 250\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_params_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    253\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(executable, out_tree, args_flat, out_flat)\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/pjit.py:163\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   dispatch\u001b[38;5;241m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/core.py:2677\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2674\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2675\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2676\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/interpreters/batching.py:398\u001b[0m, in \u001b[0;36mBatchTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    396\u001b[0m   frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_frame(vals_in, dims_in)\n\u001b[1;32m    397\u001b[0m   batched_primitive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_primitive_batcher(primitive, frame)\n\u001b[0;32m--> 398\u001b[0m   val_out, dim_out \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m src \u001b[38;5;241m=\u001b[39m source_info_util\u001b[38;5;241m.\u001b[39mcurrent()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mmultiple_results:\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/pjit.py:1415\u001b[0m, in \u001b[0;36m_pjit_batcher\u001b[0;34m(insert_axis, spmd_axis_name, axis_size, axis_name, main_type, vals_in, dims_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline)\u001b[0m\n\u001b[1;32m   1407\u001b[0m in_shardings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m   1408\u001b[0m     _pjit_batcher_for_sharding(i, axis_in, new_parts, mesh, aval\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m i\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis_in, i, aval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dims_in, in_shardings, new_jaxpr\u001b[38;5;241m.\u001b[39min_avals))\n\u001b[1;32m   1411\u001b[0m out_shardings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m   1412\u001b[0m     _pjit_batcher_for_sharding(o, axis_out, new_parts, mesh, aval\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m o\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis_out, o, aval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes_out, out_shardings, new_jaxpr\u001b[38;5;241m.\u001b[39mout_avals))\n\u001b[0;32m-> 1415\u001b[0m vals_out \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvals_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m  \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m  \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m  \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m  \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m  \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vals_out, axes_out\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/core.py:2677\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2674\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2675\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2676\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/jaxrunner/lib/python3.8/site-packages/jax/_src/pjit.py:1203\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1200\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[1;32m   1201\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1202\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_argnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_explicit_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["plt.close('all')\n","plt.ion()\n","\n","img = gen_img()\n","plot_img(img, mnmx, None)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"71Mi1BJ_KtqF"},"source":["# Build the Model (Batch inputs)"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1711010661948,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"nIZqd1r8KtW-","outputId":"bbe04b93-92e3-4cef-bcad-45aabb8a8b64"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'batch_stats': {'BatchNorm_0': {'mean': (784,), 'var': (784,)}},\n"," 'params': {'BatchNorm_0': {'bias': (784,), 'scale': (784,)},\n","            'Dense_0': {'kernel': (784, 784)},\n","            'Dense_1': {'kernel': (784, 10)}}}\n"]}],"source":["class DNN(nn.Module):\n","\n","    width = 28*28\n","    use_bias = True\n","    act_fn: Callable\n","\n","    @nn.compact\n","    def __call__(self, x, train=True):\n","        x = x.reshape((x.shape[0], -1))     # Normal mode\n","        x = nn.Dense(self.width, use_bias=False)(x)\n","        x = nn.BatchNorm(use_running_average=not train)(x)\n","        x = self.act_fn(x)\n","        x = nn.Dense(10, use_bias=False)(x)\n","        return x\n","        # return x / jnp.sqrt(self.width)\n","\n","\n","# 'act_fn' can be 'nn.relu'(ReLU), 'nn.activation.tanh'(tanh) or 'lambda x: x'(identity).\n","dnn = DNN(act_fn=nn.relu)\n","\n","x = jnp.zeros((batch_size, 28, 28, 1))\n","rng = jax.random.PRNGKey(42)\n","variables = dnn.init(rng, x, train=False)\n","pprint(jax.tree_map(jnp.shape, variables))\n","\n"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":18710,"status":"ok","timestamp":1711012228766,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"6O7OP-YILyD9"},"outputs":[],"source":["# Just one batch\n","class TrainState(train_state.TrainState):\n","    batch_stats: Any\n","\n","def train_step(state, lrband, batch):\n","\n","    '''MiniBatch are needed soon.'''\n","    # 'Phase_space' will fixed as 'param_vs_lr'.\n","    state = state.replace(params={\n","        'BatchNorm_0': {\n","            'bias': state.params['BatchNorm_0']['bias'],\n","            'scale': state.params['BatchNorm_0']['scale']\n","            },\n","        'Dense_0': {'kernel': jnp.array(state.params['Dense_0']['kernel']) + jnp.array(lrband[0])},\n","        'Dense_1': {'kernel': state.params['Dense_1']['kernel']}\n","    })  # new theta\n","\n","    def loss_fn(params):\n","\n","        logits, updates = state.apply_fn(\n","            {'params': params, 'batch_stats': state.batch_stats},\n","            x=batch['image'], train=True, mutable=['batch_stats']\n","        )\n","\n","        loss = optax.softmax_cross_entropy_with_integer_labels(\n","            logits=logits, labels=batch['label']\n","        ).mean()    # mean for 10-label\n","\n","        return loss, (logits, updates)\n","\n","    grad_fn = jax.value_and_grad(loss_fn, has_aux=True, allow_int=True)\n","    (loss, (logits, updates)), grads = grad_fn(state.params)\n","    state = state.apply_gradients(grads=grads)\n","    state = state.replace(batch_stats=updates['batch_stats'])\n","    # metrics = {\n","    #     'loss': loss.mean(),\n","    #     'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label'])\n","    # }\n","    return state, loss.mean()   # mean for tiled batch\n","\n","\n","@partial(jax.vmap, in_axes=(None, 0, None))\n","def ready_to_train(num_epochs, lrband, batch):\n","    # Initialize\n","    variables = dnn.init(jax.random.PRNGKey(42), jnp.ones((batch_size, 28, 28, 1)))\n","    # The states are vmapped by lrband.\n","    state = TrainState.create(\n","        apply_fn=dnn.apply,\n","        params=variables['params'],\n","        batch_stats=variables['batch_stats'],     ###############################\n","        tx=optax.sgd(lrband[1])     # learning rates, applied 'hparams_f'\n","        )\n","\n","    loss_archive = []\n","    for _ in range(num_epochs):\n","        state, loss = train_step(state, lrband, batch)\n","        loss_archive.append(loss)\n","\n","    # return convergence_measure_vmap(jnp.stack(loss_archive, axis=-1))\n","    return loss_archive\n","\n","\n","@jax.jit\n","@partial(jax.vmap, in_axes=(0,), out_axes=0)\n","def convergence_measure_vmap(v, max_val=1e6):\n","    '''v is loss.'''\n","\n","    fin = jnp.isfinite(v)\n","    v = v*fin + max_val*(1-fin)\n","\n","    v /= v[0]\n","    exceeds = (v > max_val)\n","    v = v*(1-exceeds) + max_val*exceeds\n","\n","    converged = (jnp.mean(v[-20:]) < 1)\n","    return jnp.where(converged, -jnp.sum(v), jnp.sum(1/v))\n","\n","\n","def train(num_epochs, lrband, batch, lrbatch=100):\n","    bs = lrband.shape[0]\n","    if bs > lrbatch:\n","        return jnp.concatenate(\n","            (train(num_epochs, lrband[:bs//2], batch, lrbatch),\n","            train(num_epochs, lrband[bs//2:], batch, lrbatch)),\n","        axis=0)\n","    return ready_to_train(num_epochs, lrband, batch)\n","\n","\n","\n","\n","losses = train(100, lrband[:4, :], batch)\n","V = convergence_measure_vmap(jnp.stack(losses, axis=-1))\n","# V = V.reshape((resolution, resolution))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":906},"executionInfo":{"elapsed":1721,"status":"error","timestamp":1711019948645,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"OhnlvgBLSLO8","outputId":"b4347f57-f409-4ab4-c18e-ec51f04d2b96"},"outputs":[{"ename":"NameError","evalue":"name 'train_state' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Just one batch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainState\u001b[39;00m(\u001b[43mtrain_state\u001b[49m\u001b[38;5;241m.\u001b[39mTrainState):\n\u001b[1;32m      3\u001b[0m     batch_stats: Any\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@with_mesh\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(state, lrband, batch):\n","\u001b[0;31mNameError\u001b[0m: name 'train_state' is not defined"]}],"source":["# Just one batch\n","class TrainState(train_state.TrainState):\n","    batch_stats: Any\n","\n","@with_mesh\n","@jax.jit\n","def train_step(state, lrband, batch):\n","\n","    '''MiniBatch are needed soon.'''\n","    # 'Phase_space' will fixed as 'param_vs_lr'.\n","    state = state.replace(params={\n","        'BatchNorm_0': {\n","            'bias': state.params['BatchNorm_0']['bias'],\n","            'scale': state.params['BatchNorm_0']['scale']\n","            },\n","        'Dense_0': {'kernel': jnp.array(state.params['Dense_0']['kernel']) + jnp.array(lrband[0])},\n","        'Dense_1': {'kernel': state.params['Dense_1']['kernel']}\n","    })  # new theta\n","\n","    @with_mesh\n","    @jax.jit\n","    def loss_fn(params):\n","\n","        logits, updates = state.apply_fn(\n","            {'params': params, 'batch_stats': state.batch_stats},\n","            x=batch['image'], train=True, mutable=['batch_stats']\n","        )\n","\n","        loss = optax.softmax_cross_entropy_with_integer_labels(\n","            logits=logits, labels=batch['label']\n","        ).mean()    # mean for 10-label\n","\n","        return loss, (logits, updates)\n","\n","    state_spec = nn.get_partition_spec(state)\n","\n","    batch = jax.lax.with_sharding_constraint(batch, P(('x', 'y')))\n","    grad_fn = jax.value_and_grad(loss_fn, has_aux=True, allow_int=True)\n","    (loss, (logits, updates)), grads = grad_fn(state.params)\n","    grads = jax.lax.with_sharding_constraint(grads, state_spec.params)\n","\n","    state = state.apply_gradients(grads=grads)\n","    state = state.replace(batch_stats=updates['batch_stats'])\n","    state = jax.lax.with_sharding_constraint(state, state_spec)\n","    # metrics = {\n","    #     'loss': loss.mean(),\n","    #     'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label'])\n","    # }\n","    return state, loss.mean()   # mean for tiled batch\n","\n","\n","@with_mesh\n","@partial(jax.jit, static_argnums=(0, ))\n","def create_state(module, lr):\n","    variables = module.init(jax.random.PRNGKey(1), jnp.ones((batch_size, 28, 28, 1)), train=False)\n","    state = TrainState.create(\n","        apply_fn=module.apply,\n","        params=variables['params'],\n","        batch_stats=variables['batch_stats'],\n","        tx=optax.sgd(lr)\n","    )\n","    state = jax.tree_map(jnp.asarray, state)\n","    state_spec = nn.get_partition_spec(state)\n","    state = with_sharding_constraint(state, state_spec)\n","    return state\n","\n","@with_mesh\n","@partial(jax.vmap, in_axes=(None, 0, None))\n","def ready_to_train(num_epochs, lrband, batch):\n","    # Initialize\n","    variables = dnn.init(jax.random.PRNGKey(42), jnp.ones((batch_size, 28, 28, 1)))\n","    # The states are vmapped by lrband.\n","    # state = TrainState.create(\n","    #     apply_fn=dnn.apply,\n","    #     params=variables['params'],\n","    #     batch_stats=variables['batch_stats'],     ###############################\n","    #     tx=optax.sgd(lrband[1])     # learning rates, applied 'hparams_f'\n","    #     )\n","    state = create_state(dnn, lrband[1])\n","\n","\n","    loss_archive = []\n","    for _ in range(num_epochs):\n","        state, loss = train_step(state, lrband, batch)\n","        loss_archive.append(loss)\n","\n","    # return convergence_measure_vmap(jnp.stack(loss_archive, axis=-1))\n","    return loss_archive\n","\n","@with_mesh\n","@jax.jit\n","@partial(jax.vmap, in_axes=(0,), out_axes=0)\n","def convergence_measure_vmap(v, max_val=1e6):\n","    '''v is loss.'''\n","\n","    fin = jnp.isfinite(v)\n","    v = v*fin + max_val*(1-fin)\n","\n","    v /= v[0]\n","    exceeds = (v > max_val)\n","    v = v*(1-exceeds) + max_val*exceeds\n","\n","    converged = (jnp.mean(v[-20:]) < 1)\n","    return jnp.where(converged, -jnp.sum(v), jnp.sum(1/v))\n","\n","\n","def train(num_epochs, lrband, batch, lrbatch=100):\n","    bs = lrband.shape[0]\n","    if bs > lrbatch:\n","        return jnp.concatenate(\n","            (train(num_epochs, lrband[:bs//2], batch, lrbatch),\n","            train(num_epochs, lrband[bs//2:], batch, lrbatch)),\n","        axis=0)\n","    return ready_to_train(num_epochs, lrband, batch)\n","\n","\n","\n","\n","losses = train(100, lrband[:4, :], batch)\n","V = convergence_measure_vmap(jnp.stack(losses, axis=-1))\n","# V = V.reshape((resolution, resolution))"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1311,"status":"ok","timestamp":1711015505647,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"iv8wMYy0eGv5","outputId":"847328f1-ec00-4874-f7d5-b9a55b2b4cd7"},"outputs":[{"data":{"text/plain":["TrainState(step=Array(0, dtype=int64, weak_type=True), apply_fn=<bound method Module.apply of DNN(\n","    # attributes\n","    act_fn = relu\n",")>, params={'BatchNorm_0': {'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0.], dtype=float32), 'scale': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1.], dtype=float32)}, 'Dense_0': {'kernel': Array([[ 0.00498469,  0.00231474,  0.06859255, ..., -0.02025034,\n","        -0.04201506, -0.0472819 ],\n","       [-0.00424231,  0.05505632, -0.03718623, ..., -0.01121093,\n","        -0.03037723, -0.04411852],\n","       [-0.01607657,  0.01104234, -0.00955781, ...,  0.0646538 ,\n","        -0.06771695,  0.04100427],\n","       ...,\n","       [-0.0782944 , -0.01209593,  0.00237653, ..., -0.04981305,\n","        -0.02322986,  0.06699233],\n","       [-0.01659886,  0.02858319,  0.07214794, ..., -0.04044611,\n","         0.03076256, -0.06213304],\n","       [-0.0170188 ,  0.00798702, -0.02792067, ..., -0.03711143,\n","         0.02464412, -0.00377197]], dtype=float32)}, 'Dense_1': {'kernel': Array([[-9.2863599e-03,  1.8335752e-02, -4.3273516e-02, ...,\n","        -1.0572814e-05,  6.9018267e-02,  7.5507887e-02],\n","       [ 5.9394599e-03,  2.1335334e-02, -3.4327579e-03, ...,\n","         1.4471377e-03, -3.5400558e-02, -3.0322867e-02],\n","       [ 3.4985892e-02, -3.2346437e-04,  3.6363252e-02, ...,\n","         4.9866252e-02,  5.7791262e-03, -3.0419119e-02],\n","       ...,\n","       [ 3.4718104e-02, -6.0815117e-03,  1.9304134e-02, ...,\n","        -2.8182900e-02,  1.7499350e-02,  1.4638156e-02],\n","       [ 3.7380625e-02,  3.9515849e-02,  1.7153893e-02, ...,\n","        -5.1872987e-02, -2.1163596e-02, -9.2661288e-03],\n","       [-1.3792817e-02, -7.7910043e-02,  3.6074437e-02, ...,\n","         3.7717000e-02, -3.1503905e-02, -1.1257224e-04]], dtype=float32)}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7bc195b9f520>, update=<function chain.<locals>.update_fn at 0x7bc195b9f640>), opt_state=(EmptyState(), EmptyState()), batch_stats={'BatchNorm_0': {'mean': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0.], dtype=float32), 'var': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1.], dtype=float32)}})"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["# https://www.kaggle.com/code/aakashnain/tf-jax-tutorials-part-8-vmap-pmap\n","create_state(dnn, lrband[0][0])"]},{"cell_type":"markdown","metadata":{"id":"_C0N7RxIc3OG"},"source":["# Initialize the Model"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1710913320681,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"dqdGKSmzUnFC","outputId":"5a2701d2-11c4-4dd8-c1ae-c6cb994f4254"},"outputs":[{"name":"stdout","output_type":"stream","text":["TrainState(step=(),\n","           apply_fn=<bound method Module.apply of DNN(\n","    # attributes\n","    act_fn = relu\n",")>,\n","           params={'BatchNorm_0': {'bias': (10,), 'scale': (10,)},\n","                   'Dense_0': {'bias': (784,), 'kernel': (784, 784)},\n","                   'Dense_1': {'bias': (10,), 'kernel': (784, 10)}},\n","           tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x79c624207250>, update=<function chain.<locals>.update_fn at 0x79c624206b90>),\n","           opt_state=(EmptyState(), EmptyState()),\n","           batch_stats={'BatchNorm_0': {'mean': (10,), 'var': (10,)}})\n"]}],"source":["params = variables['params']\n","batch_stats = variables['batch_stats']\n","y, updates = DNN.apply(\n","    {'params': params, 'batch_stats': batch_stats},\n","    x, mutable=['batch_stats']\n",")\n","batch_stats = updates['batch_stats']\n","\n","# class TrainState(TrainState):\n","    # batch_stats: Any\n","\n","@with_mesh\n","@partial(jax.jit, static_argnums=(0,))\n","def create_state(module):\n","    variables = module.init(jax.random.PRNGKey(1), jnp.ones((1, 28, 28, 1)), train=True)\n","    state = TrainState.create(\n","        apply_fn=module.apply,\n","        params=params,\n","        batch_stats=batch_stats,\n","        tx=optax.sgd(1e-3)\n","    )\n","    state = jax.tree_map(jnp.asarray, state)\n","    state_spec = nn.get_partition_spec(state)\n","    state = with_sharding_constraint(state, state_spec)\n","    return state\n","\n","state = create_state(DNN)\n","pprint(jax.tree_map(jnp.shape, state))"]},{"cell_type":"markdown","metadata":{"id":"lScbaG0WdOXW"},"source":["# Parallelize"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710913320681,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"HMnPdzrlcF4L"},"outputs":[],"source":["# Train step\n","@with_mesh\n","@jax.jit\n","def train_step(state, batch):\n","    def loss_fn(params):\n","        logits, updates = state.apply_fn(\n","            {'params': params, 'batch_stats': batch_stats},\n","            x=batch['image'], train=True, mutable=['batch_stats']\n","        )\n","        loss = optax.softmax_cross_entropy_with_integer_labels(\n","        logits=logits, labels=batch['label']\n","        ).mean()\n","        return loss, (logits, updates)\n","\n","    state_spec = nn.get_partition_spec(state)\n","\n","    batch = jax.lax.with_sharding_constraint(batch, P(('x', 'y')))\n","    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n","    (loss, (logits, updates)), grads = grad_fn(state.params)\n","    grads = jax.lax.with_sharding_constraint(grads, state_spec.params)\n","\n","    state = state.apply_gradients(grads=grads)\n","    state = state.replace(batch_stats=updates['batch_stats'])\n","\n","    logs = {\n","        'loss': loss,\n","        'accuracy': (jnp.argmax(logits, axis=-1) == batch['label']).mean()\n","    }\n","    state = jax.lax.with_sharding_constraint(state, state_spec)\n","\n","    return state, logs"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85992,"status":"ok","timestamp":1710913406670,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"lQ4jIyuFFPzQ","outputId":"f9260fe6-299c-463c-b4c2-44abbcb7e17e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 3750/3750 [01:25<00:00, 43.77it/s]\n"]}],"source":["from tqdm import tqdm\n","\n","state = create_state(DNN)\n","history = []\n","total_steps = train_ds.cardinality().numpy()\n","\n","mn1, mx1, mn2, mx2 = mnmx\n","gg1 = jnp.logspace(mn1, mx1, resolution)\n","gg2 = jnp.logspace(mn2, mx2, resolution)\n","lr0, lr1 = jnp.meshgrid(gg2, gg1)\n","lr_band = jnp.stack([lr0,ravel, lr1.ravel()], axis=-1)\n","\n","\n","for epoch in range(num_epochs):\n","    history_epoch = []\n","    for epoch, batch in enumerate(tqdm(train_ds.as_numpy_iterator(), total=total_steps)):\n","        if epoch == 0:\n","            ...\n","        state, logs = train_step(state, batch)\n","        # pprint(state['params'].keys())\n","        logs = jax.tree_map(np.asarray, logs)\n","        history_epoch.append(logs)\n","    hd = {'accuracy': [], 'loss': []}\n","    hd['accuracy'].append(np.mean([he['accuracy'] for he in history_epoch]))\n","    hd['loss'].append(np.mean([he['loss'] for he in history_epoch]))\n","    history.append(hd)\n","\n","    break"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1710913967667,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"Qo8SCWrmGSLA","outputId":"e30f9873-6dca-462a-86c3-5072f5bb95c8"},"outputs":[{"data":{"text/plain":["(784, 784)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["state.params['Dense_0']['kernel'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-_og1ZAakSl"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMY6sOuQO34iF+NveNkDY4k","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
