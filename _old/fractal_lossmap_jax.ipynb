{"cells":[{"cell_type":"markdown","metadata":{"id":"zG3VF61YwGuT"},"source":["# Installing packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28506,"status":"ok","timestamp":1711353787933,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"hOm0IcIZv_Gw","outputId":"5706040c-3351-4205-d503-6dc493254126"},"outputs":[],"source":["# !pip install porespy\n","# !pip install pypardiso\n","# !pip install ipympl"]},{"cell_type":"markdown","metadata":{"id":"InosSXqlwUL4"},"source":["# Loading libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3-C8YNzAwKQx"},"outputs":[],"source":["# GPU settings\n","import os\n","os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=4'\n","\n","# Computation settings\n","import jax\n","from jax import lax, random, config, numpy as jnp\n","config.update('jax_enable_x64', True)   # for double precision, but cannot run convolution!\n","from jax.sharding import Mesh, PartitionSpec, NamedSharding\n","from jax.experimental import mesh_utils\n","from jax.lax import with_sharding_constraint\n","import optax\n","import orbax\n","import porespy as ps\n","import numpy as np\n","\n","# Dataset settings\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","# Plotting settings\n","import matplotlib\n","# from google.colab import output\n","# output.enable_custom_widget_manager()\n","%matplotlib ipympl\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.animation as animation\n","from matplotlib import cm\n","\n","# ETC\n","import pickle\n","import shutil\n","import time\n","import datetime\n","from functools import partial\n","from typing import Callable, Any\n","from pprint import pprint\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"akdQkfkpAsO9"},"source":["# Hyperparams"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"yShvfJzA_8XU"},"outputs":[],"source":["# DNN\n","width = 784\n","depth = 2\n","target_dim = 10\n","num_epochs = 100\n","nonlinearity = 'relu'\n","\n","# Datset\n","batch_size = 100\n","minibatch_size = None\n","default_outer_batch_size = 100\n","\n","# Plotting\n","phase_space = 'paraminit_vs_lr'\n","mnmx = [-3, 6, -3, 6]\n","default_resolution = 1024\n","dpi = 100\n","figsize = (8, 8)\n","interactive_gui = True"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"jnFWa394BIR9"},"outputs":[],"source":["def canonical_name():\n","    \"\"\"\n","    turn hyperparameters in the previous cell into a canonical base filename to\n","    use for this experimental condition\n","    \"\"\"\n","    return f'zoom_sequence_width-{width}_depth-{depth}_datasetparamratio-{dataset_param_multiple}_minibatch-{minibatch_size}_nonlinearity-{nonlinearity}_phasespace-{phase_space}{\"_readout-probe_point\" if readout == \"probe_point\" else \"\"}'"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"DMSWoFujBMz0"},"outputs":[],"source":["if interactive_gui:\n","    ## interactive plotting\n","    # from google.colab import output\n","    # output.enable_custom_widget_manager()\n","    %matplotlib ipympl\n","else:\n","    matplotlib.use('Agg')"]},{"cell_type":"markdown","metadata":{"id":"36XJLykLBgzk"},"source":["# Modeling"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_XR9hT5mDH_k"},"outputs":[],"source":["# Inerternal computation part\n","def net(theta, X):\n","    '''Compute the node as dot-cross.'''\n","    X = X.reshape((X.shape[0], -1))\n","    for W in theta:\n","        Z = jnp.dot(X, W) / jnp.sqrt(W.shape[0])\n","        if nonlinearity == 'tanh':\n","            X = jax.nn.tanh(Z * jnp.sqrt(2))\n","        elif nonlinearity == 'relu':\n","            X = jax.nn.relu(Z) * jnp.wqrt(2)\n","        elif nonlinearity == 'identity':\n","            X = Z\n","        else:\n","            assert False\n","    return Z / jnp.sqrt(W.shape[0])\n","\n","def init(rng, width, depth):\n","    '''Initialize the DNN. Initial weights are defined as rng.'''\n","    rng = jax.random.split(rng, depth)\n","    theta = []\n","    out_width = width\n","    for i in range(depth):\n","        if i == depth - 1:\n","            out_width = target_dim\n","        W = jax.random.normal(rng[i], (width, out_width))\n","        theta.append(W)\n","    return theta\n","\n","def loss(theta, X, Y):\n","    Z = net(theta, X)\n","    return jnp.mean((Z-Y) ** 2)\n","\n","\n","# Learning rate selection\n","def hparams_f(hparams, theta):\n","    lr = []\n","    for i, t in enumerate(theta):\n","        lr.append(hparams[i % len(hparams)])\n","    return lr\n","\n","################################### CPU ########################################\n","# # Training step definition\n","# @partial(jax.jit, donate_argnums=(0, 1))\n","# @partial(jax.vmap, in_axes=(None, 0, 0, None, None), out_axes=(0, 0))\n","# def train_step(rng, theta, hparams, X, Y):\n","#     if phase_space == 'lr_vs_lr':\n","#         learning_rates = hparams_f(hparams, theta)\n","#     elif phase_space == 'paraminit_vs_lr':\n","#         new_theta = []\n","#         for i, t in enumerate(theta):\n","#             # First weight will added learning rate. It looks like ont-step updated.\n","#             # But this time, this learning rate is used once, new learning rate will be used other steps.\n","#             if i == 0:\n","#                 t += hparams[0]\n","#             new_theta += [t]\n","#         theta = new_theta\n","#         learning_rates = hparams_f([hparams[1]], theta)\n","#     else:\n","#         assert False, f\"Invalid phase space '{phase_space}'. Use 'lr_vs_vs' or 'paramint_vs_lr'.\"\n","\n","#     # Minibatch division (< batch_size)\n","#     # You can reduce computing cost if using minibatch.\n","#     # The loss will be computed using all-in batch, but the grad will be computed using some minibatch.\n","#     # So, updating can be weird when minibatch is very low.\n","#     if minibatch_size is None:\n","#         _loss, _grad = jax.value_and_grad(loss)(theta, X, Y)\n","#     else:\n","#         idx = jax.random.randint(rng, (minibatch_size,), 0, X.shape[0])\n","#         _X, _Y = X[idx], Y[idx]\n","#         _loss = loss(theta, X, Y)\n","#         _grad = jax.grad(loss)(theta, _X, _Y)\n","\n","#     return jax.tree_map(lambda t, g, lr: t - lr * g, theta, _grad, learning_rates), _loss\n","\n","################################################################################\n","\n","# Training step definition\n","# @partial(jax.jit, donate_argnums=(0, 1))\n","@partial(jax.vmap, in_axes=(None, 0, 0, None, None), out_axes=(0, 0))\n","def train_step(rng, theta, hparams, X, Y):\n","    new_theta = []\n","    for i, t in enumerate(theta):\n","        # First weight will added learning rate. It looks like ont-step updated.\n","        # But this time, this learning rate is used once, new learning rate will be used other steps.\n","        if i == 0:\n","            t += hparams[0]\n","        new_theta += [t]\n","    theta = new_theta\n","    learning_rates = hparams_f([hparams[1]], theta)\n","\n","    @partial(jax.pmap, axis_name='num_devices')\n","    def updates(theta, X, Y, lr):\n","        _loss, _grad = jax.value_and_grad(loss)(theta, X, Y)\n","        _grad = jax.lax.pmean(_grad, axis_name='num_devices')\n","        _loss = jax.lax.pmean(_loss, axis_name='num_devices')\n","        return jax.tree_map(lambda t, g, lr: t - lr * g, theta, _grad, learning_rates), _loss\n","    return updates(theta, X, Y, learning_rates)\n","\n","def eval_step(theta, X, Y):\n","    _val = net(theta, X)\n","    _loss = loss(theta, X, Y)\n","    return _val, _loss\n","\n","def train_n_test(theta, hparams, X, Y, tX, tY, num_steps, outer_batch_size=None, **kwargs):\n","\n","    # Random seed and ETC\n","    rng = jax.random.PRNGKey(42)\n","    if outer_batch_size is None:\n","        outer_batch_size = default_outer_batch_size\n","\n","    # Dataset control\n","    if (X is None) & (Y is None):\n","        if \"train_ds\" in kwargs.keys():\n","            dataset = kwargs['train_ds']\n","            dataset = dataset.shuffle(buffer_size=10, seed=42).batch(batch_size).prefetch(1).as_numpy_iterator()\n","            X = [d['image'] for d in dataset]\n","            Y = [d['label'] for d in dataset]\n","    if (tX is None) & (tY is None):\n","        if \"test_ds\" in kwargs.keys():\n","            dataset = kwargs['test_ds']\n","            dataset = dataset.shuffle(buffer_size=10, seed=42).batch(batch_size).prefetch(1).as_numpy_iterator()\n","            tX = [d['image'] for d in dataset]\n","            tY = [d['label'] for d in dataset]\n","    if (X is None) & (Y is None) & (tX is None) & (tY is None) & (len(kwargs.keys())==0):\n","        raise Exception(\"You should give kwargs 'train_ds=..., test_ds=...' if not giving X and Y.\")\n","\n","    # Theta initialization\n","    if theta is None:\n","        theta = init(rng, width, depth)\n","\n","    # Cut off the learning rates as bite size\n","    bs = hparams.shape[0]\n","    if bs > outer_batch_size:\n","        train_loss1, test_loss1 = train_n_test(theta, hparams[:bs//2, ], X, Y, tX, tY, num_steps)\n","        train_loss2, test_loss2 = train_n_test(theta, hparams[bs//2:, ], X, Y, tX, tY, num_steps)\n","        return jnp.concatenate((train_loss1, train_loss2), axis=0), jnp.concatenate((test_loss1, test_loss2), axis=0)\n","\n","    # Train session\n","    rng = jax.random.split(rng, num_steps)\n","    loss_archive = []\n","    _theta = jax.tree_map(lambda u: jnp.tile(u, (bs,) * len(u.shape)), theta)\n","\n","    for epoch, _rng in tqdm(enumerate(rng), total=num_steps, desc='Epochs', leave=True):\n","        # Need to divide X and Y as each batch.\n","        loss_epoch = 0.\n","        for batch in tqdm(range(len(X)), desc='Batches', leave=False):\n","            _theta, _loss = train_step(_rng, _theta, hparams, X[epoch], Y[epoch])\n","            loss_epoch += _loss\n","        loss_epoch /= len(len(X))\n","        loss_archive.append(loss_epoch)\n","\n","    # Test session\n","    tloss_archive = []\n","    _, _tloss = eval_step(_theta, tX, tY)\n","    tloss_archive.append(_tloss)\n","\n","    return convergence_measure(jnp.stack(loss_archive, axis=-1)), convergence_measure(jnp.stack(tloss_archive, axis=-1))\n","\n","\n","# Compensator\n","@jax.jit\n","@partial(jax.vmap, in_axes=(0,), out_axes=0)\n","def convergence_measure(v, max_val=1e6):\n","    fin = jnp.isfinite(v)\n","    v = v * fin + max_val * (1-fin)\n","    v /= v[0]\n","    exceeds = (v > max_val)\n","    v = v * (1-exceeds) + max_val * exceeds\n","    converged = (jnp.mean(v[-20:]) < 1)\n","    return jnp.where(converged, -jnp.sum(v), jnp.sum(1/v))"]},{"cell_type":"markdown","metadata":{"id":"szLdaCvzhItA"},"source":["# Plotting and Measure the Fractal Dimension"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"GxkGz1-whK3-"},"outputs":[],"source":["# Generate the lossmap\n","def gen_img(mnmx, resolution=None):\n","    \"\"\"\n","    generate an image of the hyperparameter landscape,\n","    for a range of hyperparameter values specified by mnmx\n","    \"\"\"\n","\n","    if resolution is None:\n","        resolution = default_resolution\n","\n","    mn1, mx1, mn2, mx2 = mnmx\n","    gg1 = jnp.logspace(mn1, mx1, resolution)\n","    gg2 = jnp.logspace(mn2, mx2, resolution)\n","    lr0, lr1 = jnp.meshgrid(gg2, gg1)\n","    lr = jnp.stack([lr0.ravel(), lr1.ravel()], axis=-1)\n","\n","    V, tV = train_n_test(\n","        theta=None,\n","        hparams=lr,\n","        X=None, Y=None,\n","        tX=None, tY=None,\n","        num_steps=num_epochs,\n","        train_ds=train_ds,\n","        test_ds=test_ds\n","        )\n","\n","    return V.reshape((resolution, resolution)), tV.reshape((resolution, resolution))\n","\n","\n","# Measure the fractal dim\n","def extract_edges(X):\n","    \"\"\"\n","    define edges as sign changes in the scalar representing convergence or\n","    divergence rate -- on one side of the edge training converges,\n","    while on the other side of the edge training diverges\n","    \"\"\"\n","\n","    Y = jnp.stack((X[1:,1:], X[:-1,1:], X[1:,:-1], X[:-1,:-1]), axis=-1)\n","    Z = jnp.sign(jnp.max(Y, axis=-1)*jnp.min(Y, axis=-1))\n","    return Z<0\n","\n","def estimate_fractal_dimension(hist_video, show_plot=True):\n","    edges = [extract_edges(U[0]) for U in hist_video]\n","    box_counts = [ps.metrics.boxcount(U) for U in edges]\n","    all_images = np.concatenate([bc.slope for bc in box_counts])\n","\n","    if show_plot:\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n","        ax1.set_yscale('log')\n","        ax1.set_xscale('log')\n","        ax1.set_xlabel('box edge length')\n","        ax1.set_ylabel('number of boxes spanning phases')\n","        ax2.set_xlabel('box edge length')\n","        ax2.set_ylabel('image')\n","        ax2.set_xscale('log')\n","\n","        for bc in box_counts:\n","            ax1.plot(bc.size, bc.count,'-o')\n","            ax2.plot(bc.size, bc.slope,'-o');\n","\n","    mfd = np.median(all_images)\n","    print(f'median fractal dimension estimate {mfd}')\n","\n","    return mfd\n","\n","\n","# Interploating\n","def cdf_img(x, x_ref, buffer=0.25):\n","    \"\"\"\n","    rescale x, relative to x_ref (x_ref is often the same as x), to achieve a uniform\n","    distribution over values with positive and negative intensities, but also to\n","    preserve the sign of x. This makes for a visualization that shows more\n","    structure.\n","    \"\"\"\n","    u = jnp.sort(x_ref.ravel())\n","    num_neg = jnp.sum(u<0)\n","    num_nonneg = u.shape[0] - num_neg\n","    v = jnp.concatenate((jnp.linspace(-1,-buffer,num_neg), jnp.linspace(buffer,1,num_nonneg)), axis=0)\n","    y = jnp.interp(x, u, v)\n","    return -y\n","\n","\n","# Notation\n","def truncate_sci_notation(numbers):\n","    \"\"\"\n","    keeping enough significant digits that the\n","    numbers disagree in four digits\n","    \"\"\"\n","\n","    # Convert numbers to scientific notation\n","    n1_sci, n2_sci = \"{:.15e}\".format(numbers[0]), \"{:.15e}\".format(numbers[1])\n","\n","    # Extract the significant parts and exponents\n","    sig_n1, exp_n1 = n1_sci.split('e')\n","    sig_n2, exp_n2 = n2_sci.split('e')\n","\n","    # Find the first position at which they disagree\n","    min_len = min(len(sig_n1), len(sig_n2))\n","    truncate_index = min_len\n","\n","    for i in range(min_len):\n","        if (sig_n1[i] != sig_n2[i]) or (exp_n1 != exp_n2):\n","            # +4 accounts for 4 digits after the first disagreement\n","            truncate_index = i + 4\n","            if i == 0:\n","                truncate_index += 1 # Account for decimal point\n","        break\n","\n","    exp_n1 = exp_n1[0] + exp_n1[2]\n","    exp_n2 = exp_n2[0] + exp_n2[2]\n","    if (exp_n1 == \"+00\") and (exp_n2 == \"+00\"):\n","        # don't bother with scientific notation if exponent is 0\n","        return [sig_n1[:truncate_index], sig_n2[:truncate_index]]\n","\n","    # Truncate and reconstruct the scientific notation\n","    truncated_n1 = \"{}e{}\".format(sig_n1[:truncate_index], exp_n1)\n","    truncated_n2 = \"{}e{}\".format(sig_n2[:truncate_index], exp_n2)\n","\n","    return [truncated_n1, truncated_n2]\n","\n","def tickslabels(mnmx):\n","    return mnmx, truncate_sci_notation(10.**np.array(mnmx))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"MrJfi2LYjceH"},"outputs":[],"source":["# Interactive plotting\n","cids = []\n","click_event = [None]\n","\n","def onclick(event):\n","  click_event[0] = (event.xdata, event.ydata)\n","\n","def onrelease(event, fig, im, rect, mnmx, img, recalculate_image=True):\n","  if click_event[0] is None:\n","    return\n","\n","  e0 = [click_event[0][0], event.xdata]\n","  e1 = [click_event[0][1], event.ydata]\n","\n","  for v in e0+e1:\n","    if v is None:\n","      return\n","\n","  newmnmx = [np.min(e1), np.max(e1), np.min(e0), np.max(e0)]\n","\n","  min_w = (mnmx[1] - mnmx[0])/20\n","  if newmnmx[1] - newmnmx[0] < min_w:\n","    c = (newmnmx[1] + newmnmx[0])/2.\n","    newmnmx[0] = c - min_w/2\n","    newmnmx[1] = c + min_w/2\n","  min_w = (mnmx[3] - mnmx[2])/20\n","  if newmnmx[1] - newmnmx[0] < min_w:\n","    c = (newmnmx[3] + newmnmx[2])/2.\n","    newmnmx[2] = c - min_w/2\n","    newmnmx[3] = c + min_w/2\n","\n","  for v in newmnmx:\n","    if v is None:\n","      return\n","  plot_img(img, mnmx, newmnmx, fig=fig, im=im, rect=rect)\n","  plt.draw()\n","\n","  if recalculate_image:\n","    click_event[0] = None\n","    mnmx = newmnmx\n","    img = gen_img(mnmx)\n","    plot_img(img, mnmx, None, fig=fig, im=im, rect=rect)\n","\n","def plot_img(image, mnmx, newmnmx=None, fig=None, im=None, rect=None,\n","             handler=True, savename=None,\n","             reference_scale=None,\n","             cmap='Spectral',\n","             title=\"\"\n","             ):\n","  mn1, mx1, mn2, mx2 = mnmx\n","\n","  if reference_scale is None:\n","    reference_scale = image\n","\n","  image = cdf_img(image, reference_scale)\n","\n","  ax1 = None\n","  if fig is None:\n","    fig, (ax1) = plt.subplots(figsize=figsize, dpi=dpi)\n","    im = ax1.imshow(image,\n","                    extent=[mn2, mx2, mn1, mx1],\n","                    origin='lower',\n","                    vmin=-1, vmax=1,\n","                    cmap=cmap,\n","                    aspect='auto',\n","                    interpolation='nearest'\n","                    )\n","    batch_text = f\"{batch_size}\" + \"\" if minibatch_size is None else f\"(mini-{minibatch_size})\"\n","    title = f'Trainability dependence on parameter initialization and learning rate\\n1 hidden layer, {nonlinearity}, {batch_text}'\n","    if not title == \"\":\n","      plt.title(title)\n","    if phase_space == 'lr_vs_lr':\n","      ax1.set_ylabel('Output layer learning rate')\n","      ax1.set_xlabel('Input layer learning rate')\n","    elif phase_space == 'paraminit_vs_lr':\n","      ax1.set_ylabel('Learning rate')\n","      ax1.set_xlabel('Input layer weight offset')\n","\n","    rect = patches.Rectangle((mn2, mn1), mx2-mn2, mx1-mn1, linewidth=1, edgecolor='r', facecolor='none')\n","    ax1.add_patch(rect)\n","\n","  im.set_extent([mn2, mx2, mn1, mx1])\n","  im.set_data(image)\n","\n","  # Set the new tick positions on the x-axis\n","  aaxx = plt.gca()\n","  aaxx.set_xticks(*tickslabels([mn2, mx2]))\n","  aaxx.set_yticks(*tickslabels([mn1, mx1]), rotation=90)\n","\n","  labels = aaxx.get_xticklabels()\n","  labels[0].set_horizontalalignment('left')\n","  labels[1].set_horizontalalignment('right')\n","  labels = aaxx.get_yticklabels()\n","  labels[0].set_verticalalignment('bottom')\n","  labels[1].set_verticalalignment('top')\n","\n","  if handler and (newmnmx is None):\n","    image_history.append((image, mnmx))\n","\n","  if newmnmx:\n","    mn1, mx1, mn2, mx2 = newmnmx\n","  rect.set_xy((mn2, mn1))\n","  rect.set_width(mx2-mn2)\n","  rect.set_height(mx1-mn1)\n","\n","  if handler:\n","    while len(cids) > 0:\n","      fig.canvas.mpl_disconnect(cids.pop())\n","\n","    def onrelease_partial(event):\n","      return onrelease(event, fig, im, rect, mnmx, img)\n","    def onmotion_partial(event):\n","      return onrelease(event, fig, im, rect, mnmx, img, recalculate_image=False)\n","\n","    cids.append(fig.canvas.mpl_connect('button_press_event', onclick))\n","    cids.append(fig.canvas.mpl_connect('button_release_event', onrelease_partial))\n","    # cids.append(fig.canvas.mpl_connect('motion_notify_event', onmotion_partial))\n","\n","  plt.tight_layout()\n","\n","  plt.draw()\n","\n","  if savename:\n","    plt.savefig(savename)\n","\n","  return fig, ax1, im\n","\n","\n","# Animating zoom sequences and interpolating between frames\n","def zoom_out_sequence(hist_final, growth_factor=2., max_scale=6):\n","  \"\"\"\n","  generate a sequence of (image, bounds) zooming out from the (image, bounds) in hist_final\n","  \"\"\"\n","\n","  image, mnmx = hist_final\n","\n","  cT = np.array([(mnmx[0] + mnmx[1])/2., (mnmx[2] + mnmx[3])/2.])\n","  wT = np.array([mnmx[1] - mnmx[0], mnmx[3] - mnmx[2]])\n","\n","  hist = [(image, mnmx)]\n","  w_scale = 1.\n","  while np.min(wT * w_scale) < max_scale:\n","    w_scale *= 2\n","    mnmx = [\n","        cT[0] - w_scale * wT[0]/2.,\n","        cT[0] + w_scale * wT[0]/2.,\n","        cT[1] - w_scale * wT[1]/2.,\n","        cT[1] + w_scale * wT[1]/2.,\n","    ]\n","    hist.insert(0, (np.zeros((2,2)), mnmx))\n","\n","  return hist\n","\n","def increase_resolution(history, target_res):\n","  \"\"\"\n","  Increase the resolution of images of a fractal landscape that we've already\n","  generated.\n","\n","  Find the first entry in history with resolution below target_res, and increase\n","  its resolution. If all images are already at least the target resolution,\n","  return False.\n","  \"\"\"\n","\n","  new_h = []\n","  for ii in range(len(history)):\n","    h = history[ii]\n","    image, mnmx = h\n","    if image.shape[0] < target_res:\n","      current_time = datetime.datetime.now()\n","      print( f\"increasing resolution of {ii} / {len(history)} at {current_time}, current resolution is {image.shape}\")\n","      image = gen_img(mnmx, resolution=target_res)\n","      history[ii] = (image, mnmx)\n","      return True\n","  return False\n","\n","def interpolate_history(hist1, hist2, alpha):\n","  \"\"\"\n","  get the mnmx (hyperparameter bounding box) value for a fraction alpha between\n","  two images\n","  \"\"\"\n","\n","  _, mnmx1 = hist1\n","  _, mnmx2 = hist2\n","\n","  if alpha == 0:\n","    # avoid NaNs on very last frame\n","    return mnmx1\n","\n","  w1 = np.array([mnmx1[1] - mnmx1[0], mnmx1[3] - mnmx1[2]])\n","  w2 = np.array([mnmx2[1] - mnmx2[0], mnmx2[3] - mnmx2[2]])\n","  c1 = np.array([(mnmx1[0] + mnmx1[1])/2, (mnmx1[2] + mnmx1[3])/2])\n","  c2 = np.array([(mnmx2[0] + mnmx2[1])/2, (mnmx2[2] + mnmx2[3])/2])\n","\n","  gamma = np.exp((1-alpha)*0 + alpha*np.log(w2/w1))\n","\n","  # ct = cstar + (c1 - cstar)*gamma\n","  # c1 = cstar + (c1 - cstar)*1\n","  # c2 = cstar + (c1 - cstar)*w2/w1\n","  cstar = (c2 - c1*w2/w1) / (1 - w2 / w1)\n","\n","  ct = cstar + (c1 - cstar)*gamma\n","  hwt = gamma*w1\n","\n","  return [ct[0] - hwt[0]/2, ct[0] + hwt[0]/2, ct[1] - hwt[1]/2, ct[1] + hwt[1]/2]\n","\n","\n","def em(extent_rev):\n","  return [extent_rev[2], extent_rev[3], extent_rev[0], extent_rev[1]]\n","\n","def make_animator(history, timesteps_per_transition=60, reference_scale=None, cmap='Spectral'):\n","\n","  fig, ax, im1 = plot_img(history[0][0], history[0][1], newmnmx=None,\n","                          handler=False, reference_scale=reference_scale, cmap=cmap)\n","\n","  im2 = ax.imshow(\n","      jnp.zeros_like(history[1][0]), extent=em(history[1][1]), origin='lower',\n","      vmin = -1, vmax = 1,\n","      cmap=cmap,\n","      aspect='auto',\n","      interpolation='nearest'\n","      )\n","\n","  im3 = ax.imshow(\n","      jnp.zeros_like(history[1][0]), extent=em(history[1][1]), origin='lower',\n","      vmin = -1, vmax = 1,\n","      cmap=cmap,\n","      aspect='auto',\n","      interpolation='nearest'\n","      )\n","\n","  def animate(n):\n","    hist_index = n // timesteps_per_transition\n","    alpha = (n % timesteps_per_transition) / timesteps_per_transition\n","\n","    hist1 = history[hist_index]\n","    if hist_index >= len(history)-1:\n","      hist2 = hist1 # very last frame\n","    else:\n","      hist2 = history[hist_index+1]\n","    if hist_index >= len(history)-2:\n","      hist3 = hist2 # very last frame\n","    else:\n","      hist3 = history[hist_index+2]\n","\n","    lims = interpolate_history(hist1, hist2, alpha)\n","\n","    # interpolation scheme for image restretch / colormap\n","    alpha_area = jnp.sin(alpha*np.pi/2)**2\n","\n","    print(f'frame {n} / {timesteps_per_transition*len(history)}, zoom step {hist_index} / {len(history)}', end='\\r', flush=True)\n","\n","    img_1 = (1-alpha_area)*cdf_img(hist1[0], hist1[0]) + alpha_area*cdf_img(hist1[0], hist2[0])\n","    img_2 = (1-alpha_area)*cdf_img(hist2[0], hist1[0]) + alpha_area*cdf_img(hist2[0], hist2[0])\n","    img_3 = (1-alpha_area)*cdf_img(hist3[0], hist1[0]) + alpha_area*cdf_img(hist3[0], hist2[0])\n","\n","    im1.set_data(img_1)\n","    im1.set_extent(em(hist1[1]))\n","    im2.set_data(img_2)\n","    im2.set_extent(em(hist2[1]))\n","    im3.set_data(img_3)\n","    im3.set_extent(em(hist3[1]))\n","    im3.set_alpha(alpha)\n","\n","    ax.set_ylim(lims[0], lims[1])\n","    ax.set_xlim(lims[2], lims[3])\n","\n","    # Set the new tick positions\n","    ax.set_xticks(*tickslabels([lims[2], lims[3]]))\n","    ax.set_yticks(*tickslabels([lims[0], lims[1]]), rotation=90)\n","\n","    labels = ax.get_xticklabels()\n","    labels[0].set_horizontalalignment('left')\n","    labels[1].set_horizontalalignment('right')\n","    labels = ax.get_yticklabels()\n","    labels[0].set_verticalalignment('bottom')\n","    labels[1].set_verticalalignment('top')\n","\n","    return fig,\n","\n","  anim = animation.FuncAnimation(fig,animate,frames=timesteps_per_transition*(len(history)-1)+1, repeat=False)\n","  return anim"]},{"cell_type":"markdown","metadata":{"id":"esuAL5tAit2N"},"source":["# Generating the lossmap"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"B2R7QrxQladt"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-26 08:41:31.781176: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n"]}],"source":["train_ds = tfds.load('mnist', split='train')\n","test_ds = tfds.load('mnist', split='test')\n","\n","def data_normalize(ds):\n","    return ds.map(lambda sample: {\n","        'image': tf.cast(sample['image'], tf.float32) / 256.,\n","        'label': sample['label']\n","    })\n","\n","train_ds = data_normalize(train_ds)\n","test_ds = data_normalize(test_ds)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"H3r5KDHEhEeu"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a0a27898e12466d9a52f35c8a4c38eb","version_major":2,"version_minor":0},"text/plain":["Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1ad8abc719841688f1714dbd03eadeb","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/600 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the lossmap\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_img, test_img \u001b[38;5;241m=\u001b[39m \u001b[43mgen_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnmx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m jnp\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/npy/train_lossmap.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, train_img)\n\u001b[1;32m      4\u001b[0m jnp\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/npy/test_lossmap.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, test_img)\n","Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mgen_img\u001b[0;34m(mnmx, resolution)\u001b[0m\n\u001b[1;32m     14\u001b[0m lr0, lr1 \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmeshgrid(gg2, gg1)\n\u001b[1;32m     15\u001b[0m lr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mstack([lr0\u001b[38;5;241m.\u001b[39mravel(), lr1\u001b[38;5;241m.\u001b[39mravel()], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m V, tV \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_n_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ds\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m V\u001b[38;5;241m.\u001b[39mreshape((resolution, resolution)), tV\u001b[38;5;241m.\u001b[39mreshape((resolution, resolution))\n","Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_n_test\u001b[0;34m(theta, hparams, X, Y, tX, tY, num_steps, outer_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m bs \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bs \u001b[38;5;241m>\u001b[39m outer_batch_size:\n\u001b[0;32m--> 134\u001b[0m     train_loss1, test_loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_n_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     train_loss2, test_loss2 \u001b[38;5;241m=\u001b[39m train_n_test(theta, hparams[bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:, ], X, Y, tX, tY, num_steps)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate((train_loss1, train_loss2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), jnp\u001b[38;5;241m.\u001b[39mconcatenate((test_loss1, test_loss2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_n_test\u001b[0;34m(theta, hparams, X, Y, tX, tY, num_steps, outer_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m bs \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bs \u001b[38;5;241m>\u001b[39m outer_batch_size:\n\u001b[0;32m--> 134\u001b[0m     train_loss1, test_loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_n_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     train_loss2, test_loss2 \u001b[38;5;241m=\u001b[39m train_n_test(theta, hparams[bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:, ], X, Y, tX, tY, num_steps)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate((train_loss1, train_loss2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), jnp\u001b[38;5;241m.\u001b[39mconcatenate((test_loss1, test_loss2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","    \u001b[0;31m[... skipping similar frames: train_n_test at line 134 (11 times)]\u001b[0m\n","Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_n_test\u001b[0;34m(theta, hparams, X, Y, tX, tY, num_steps, outer_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m bs \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bs \u001b[38;5;241m>\u001b[39m outer_batch_size:\n\u001b[0;32m--> 134\u001b[0m     train_loss1, test_loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_n_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     train_loss2, test_loss2 \u001b[38;5;241m=\u001b[39m train_n_test(theta, hparams[bs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:, ], X, Y, tX, tY, num_steps)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate((train_loss1, train_loss2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), jnp\u001b[38;5;241m.\u001b[39mconcatenate((test_loss1, test_loss2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_n_test\u001b[0;34m(theta, hparams, X, Y, tX, tY, num_steps, outer_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 147\u001b[0m     _theta, _loss \u001b[38;5;241m=\u001b[39m train_step(_rng, _theta, hparams, X[epoch], \u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    148\u001b[0m     loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _loss\n\u001b[1;32m    149\u001b[0m loss_epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["# Get the lossmap\n","train_img, test_img = gen_img(mnmx)\n","jnp.save('./output/npy/train_lossmap.npy', train_img)\n","jnp.save('./output/npy/test_lossmap.npy', test_img)\n"]},{"cell_type":"markdown","metadata":{"id":"Ybm7MAd-t9YT"},"source":["## Train lossmap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ei_iEeU6r2HL"},"outputs":[],"source":["# Train lossmap\n","image_history = []\n","plt.close('all')\n","plt.ion()\n","plot_img(train_img, mnmx, None)\n","plt.show()\n","\n","## WARNING!!\n","## After finishing to interact the plot,\n","## - Should do next-step immediately!!!\n","## - No plotting before making current frames,"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJCqg1pCtcXL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wMRo7x7r7D0"},"outputs":[],"source":["# Test lossmap\n","plt.close('all')\n","plt.ion()\n","plot_img(test_img, mnmx, None)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JR2MgYiSdEZw"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"tWMCKVCfvUBS"},"source":["* 내일 할거\n","\n","telegram mesg\n","\n","hyperparam rearr."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711355118640,"user":{"displayName":"J U","userId":"09528049220557542940"},"user_tz":-540},"id":"qojb13YJNOmp","outputId":"519611c8-e7d9-4482-fd6d-b1bdf31b1413"},"outputs":[{"name":"stdout","output_type":"stream","text":["[4, 2, 3] [1, 2, 3]\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64qx1rI1tp0c"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNYJRIT74GqZf5wLdv8up21","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
