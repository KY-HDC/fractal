{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=4'\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,4' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "train_ds = tfds.load('mnist', split='train')\n",
    "test_ds = tfds.load('mnist', split='test')\n",
    "\n",
    "def data_normalize(ds):\n",
    "    return ds.map(lambda sample: {\n",
    "        'image': tf.cast(sample['image'], tf.float32) / 255.,\n",
    "        'label': sample['label']\n",
    "    })\n",
    "\n",
    "train_ds = data_normalize(train_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
    "test_ds = data_normalize(test_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
    "\n",
    "total_batch = train_ds.cardinality().numpy()\n",
    "total_tbatch = test_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "resnet_kernel_init = jax.nn.initializers.variance_scaling(2.0, mode='fan_out', distribution='normal')\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    act_fn : callable  # Activation function\n",
    "    c_out : int   # Output feature size\n",
    "    subsample : bool = False  # If True, we apply a stride inside F\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # Network representing F\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    strides=(1, 1) if not self.subsample else (2, 2),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(x)\n",
    "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
    "\n",
    "        if self.subsample:\n",
    "            x = nn.Conv(self.c_out, kernel_size=(1, 1), strides=(2, 2), kernel_init=resnet_kernel_init, use_bias=False)(x)\n",
    "\n",
    "        x_out = self.act_fn(z + x)\n",
    "        return x_out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    num_classes : int\n",
    "    act_fn : callable\n",
    "    block_class : nn.Module\n",
    "    num_blocks : tuple = (3, 3, 3)\n",
    "    c_hidden : tuple = (16, 32, 64)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # A first convolution on the original image to scale up the channel size\n",
    "        x = nn.Conv(self.c_hidden[0], kernel_size=(3, 3), kernel_init=resnet_kernel_init, use_bias=False)(x)\n",
    "        if self.block_class == ResNetBlock:  # If pre-activation block, we do not apply non-linearities yet\n",
    "            x = nn.BatchNorm()(x, use_running_average=not train)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        # Creating the ResNet blocks\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            for bc in range(block_count):\n",
    "                # Subsample the first block of each group, except the very first one.\n",
    "                subsample = (bc == 0 and block_idx > 0)\n",
    "                # ResNet block\n",
    "                x = self.block_class(c_out=self.c_hidden[block_idx],\n",
    "                                     act_fn=self.act_fn,\n",
    "                                     subsample=subsample)(x, train=train)\n",
    "\n",
    "        # Mapping to classification output\n",
    "        x = x.mean(axis=(1, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(self.num_classes, use_bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.as_numpy_iterator():\n",
    "    x = batch['image']\n",
    "    y = batch['label']\n",
    "    break\n",
    "\n",
    "resnet11 = ResNet(num_classes=10, act_fn=nn.relu, block_class=ResNetBlock)\n",
    "print(nn.tabulate(resnet11, jax.random.PRNGKey(42))(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "\n",
    "# @jax.jit\n",
    "def net_test(variables, x: jnp.array):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    # T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            # k_conv0 = jnp.transpose(v['Conv_0']['kernel'], [3, 2, 0, 1])\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=True)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            # k_conv1 = jnp.transpose(v['Conv_1']['kernel'], [3, 2, 0, 1])\n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=True)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                # k_conv2 = jnp.transpose(v['Conv_2']['kernel'], [3, 2, 0, 1])\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "    # running_mu = running_mu.reshape((1, running_mu.shape[0], 1, 1))\n",
    "    # running_var = running_var.reshape((1, running_var.shape[0], 1, 1))\n",
    "\n",
    "    if on_train == True:\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        running_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        x = (x - mu) / jnp.sqrt(var + eps)\n",
    "    else:\n",
    "        x = (x - running_mu) / jnp.sqrt(running_var + eps)\n",
    "    x = gamma * x + beta\n",
    "\n",
    "    # gamma = gamma.reshape((gamma.shape[1],))\n",
    "    # beta = beta.reshape((beta.shape[1],))\n",
    "    # running_mu = running_mu.reshape((running_mu.shape[1],))\n",
    "    # running_var = running_var.reshape((running_var.shape[1],))\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "    \n",
    "# def loss_fn(variables, x, y):\n",
    "#     logits, variables = net_test(variables, x)\n",
    "#     return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "\n",
    "# @jax.jit\n",
    "# def train_oneEpoch(variables, x, y, lr):\n",
    "#     (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)  \n",
    "#     return loss, variables, grads\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(variables, x, y, lr):\n",
    "    def loss_fn(variables, x, y):\n",
    "        logits, variables = net_test(variables, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "    \n",
    "    (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (loss, logits)\n",
    "    \n",
    "def train(variables, batches, lr, epochs):\n",
    "    loss_archive, logits_archive = [], []\n",
    "    lr = jax.tree_map(lambda x: jnp.array((lr), dtype=jnp.float32), variables['params'])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        for batch in batches.as_numpy_iterator():\n",
    "            x = batch['image']\n",
    "            y = batch['label']\n",
    "            variables, (loss, logits) = update_fn(variables, x, y, lr)\n",
    "        loss_archive.append(loss)\n",
    "        logits_archive.append(logits)\n",
    "    return loss_archive, logits_archive\n",
    "\n",
    "def initialize(module, rng, x):\n",
    "    variables = module.init(jax.random.PRNGKey(rng), x)\n",
    "    variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))  # 64 * 28**2\n",
    "    # variables['params'] = jax.tree_map(lambda param: jnp.transpose(param, (3, 2, 0, 1)), variables['params'])\n",
    "    def conv_dog(kp, x):\n",
    "        kp = jax.tree_util.keystr(kp)\n",
    "        if 'Conv' in kp:\n",
    "            x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "        return x\n",
    "    variables['params'] = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "    variables['batch_stats'] = jax.tree_map(lambda stats: stats.reshape((1, stats.shape[0], 1, 1)), variables['batch_stats'])\n",
    "    return variables\n",
    "        \n",
    "\n",
    "# variables = resnet11.init(jax.random.PRNGKey(42), x)\n",
    "\n",
    "# custom_variables = variables.copy()\n",
    "# custom_variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))\n",
    "# custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['mean'] = custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['mean'].reshape((1, custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['mean'].shape[0], 1, 1))\n",
    "# custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['var'] = custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['var'].reshape((1, custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['var'].shape[0], 1, 1))\n",
    "variables = initialize(resnet11, 42, x)\n",
    "loss_archive, logits_archive = train(variables, train_ds, lr=1e-3, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "train_ds = tfds.load('mnist', split='train')\n",
    "test_ds = tfds.load('mnist', split='test')\n",
    "\n",
    "def data_normalize(ds):\n",
    "    return ds.map(lambda sample: {\n",
    "        'image': tf.cast(sample['image'], tf.float32) / 255.,\n",
    "        'label': sample['label']\n",
    "    })\n",
    "\n",
    "train_ds = data_normalize(train_ds).shuffle(buffer_size=10, seed=42).batch(1000).prefetch(1).take(1000)\n",
    "test_ds = data_normalize(test_ds).shuffle(buffer_size=10, seed=42).batch(1000).prefetch(1).take(1000)\n",
    "\n",
    "total_batch = train_ds.cardinality().numpy()\n",
    "total_tbatch = test_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.as_numpy_iterator():\n",
    "    x = batch['image']\n",
    "    y = batch['label']\n",
    "    break\n",
    "\n",
    "resnet11 = ResNet(num_classes=10, act_fn=nn.relu, block_class=ResNetBlock)\n",
    "print(nn.tabulate(resnet11, jax.random.PRNGKey(42))(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_data(data, n_devices):\n",
    "    data = data.reshape(n_devices, data.shape[0] // n_devices, *data.shape[1:])\n",
    "    return data\n",
    "\n",
    "shard_data(x, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def net_test(variables, x: jnp.array):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    \n",
    "    # input.T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=True)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=True)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "\n",
    "    if on_train == True:\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        running_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        x = (x - mu) / jnp.sqrt(var + eps)\n",
    "    else:\n",
    "        x = (x - running_mu) / jnp.sqrt(running_var + eps)\n",
    "    x = gamma * x + beta\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "    \n",
    "# @jax.jit\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(None, 0, 0, None), out_axes=(None, 0))\n",
    "def update_fn(variables, x, y, lr):\n",
    "    # @jax.jit\n",
    "    def loss_fn(variables, x, y):\n",
    "        logits, variables = net_test(variables, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "    \n",
    "    (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    # loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "    # logits = jax.lax.pmean(logits, axis_name='batch')\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (loss, logits)\n",
    "\n",
    "def train(variables, batches, lr, epochs):\n",
    "    loss_archive, logits_archive = [], []\n",
    "    lr = jax.tree_map(lambda x: jnp.array((lr), dtype=jnp.float32), variables['params'])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        for batch in batches.as_numpy_iterator():\n",
    "            x = shard_data(batch['image'], 4)\n",
    "            y = shard_data(batch['label'], 4)\n",
    "            \n",
    "            variables, (loss, logits) = update_fn(variables, x, y, lr)\n",
    "        loss_archive.append(loss)\n",
    "        logits_archive.append(logits)\n",
    "    return loss_archive, logits_archive\n",
    "\n",
    "def initialize(module, rng, x):\n",
    "    variables = module.init(jax.random.PRNGKey(rng), x)\n",
    "    variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))  # 64 * 28**2\n",
    "    # variables['params'] = jax.tree_map(lambda param: jnp.transpose(param, (3, 2, 0, 1)), variables['params'])\n",
    "    def conv_dog(kp, x):\n",
    "        kp = jax.tree_util.keystr(kp)\n",
    "        if 'Conv' in kp:\n",
    "            x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "        return x\n",
    "    variables['params'] = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "    variables['batch_stats'] = jax.tree_map(lambda stats: stats.reshape((1, stats.shape[0], 1, 1)), variables['batch_stats'])\n",
    "    return variables\n",
    "        \n",
    "\n",
    "variables = initialize(resnet11, 42, x)\n",
    "loss_archive, logits_archive = train(variables, train_ds, lr=0.01, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def net_test(variables, x: jnp.array):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    \n",
    "    # input.T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=True)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=True)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "\n",
    "    if on_train == True:\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        running_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        x = (x - mu) / jnp.sqrt(var + eps)\n",
    "    else:\n",
    "        x = (x - running_mu) / jnp.sqrt(running_var + eps)\n",
    "    x = gamma * x + beta\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "    \n",
    "# @jax.jit\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(None, 0, 0, None), out_axes=(None, 0))\n",
    "@partial(jax.vmap, in_axes=(0, None, None, None))\n",
    "def update_fn(variables, x, y, lr):\n",
    "    @jax.jit\n",
    "    def loss_fn(variables, x, y):\n",
    "        logits, variables = net_test(variables, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "    \n",
    "    (losses, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    # losses = jax.lax.pmean(loss, axis_name='batch')\n",
    "    # losses = losses.mean(axis=0)\n",
    "    # logits = jax.lax.pmean(logits, axis_name='batch')\n",
    "    # logits = losses.mean(axis=0)\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (losses, logits)\n",
    "\n",
    "def train(variables, batches, lr, epochs):\n",
    "    loss_archive, logit_archive = [], []\n",
    "    lr = jax.tree_map(lambda x: jnp.array((lr), dtype=jnp.float32), variables['params'])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        # vmapping needed\n",
    "        for batch in batches.as_numpy_iterator():\n",
    "            x = shard_data(batch['image'], 4)\n",
    "            y = shard_data(batch['label'], 4)\n",
    "            \n",
    "            variables, (losses, logits) = update_fn(variables, x, y, lr)\n",
    "        loss_archive.append(losses)\n",
    "        logit_archive.append(logits)\n",
    "    return loss_archive, logit_archive\n",
    "\n",
    "def initialize(module, rng, x):\n",
    "    variables = module.init(jax.random.PRNGKey(rng), x)\n",
    "    variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))  # 64 * 28**2\n",
    "    # variables['params'] = jax.tree_map(lambda param: jnp.transpose(param, (3, 2, 0, 1)), variables['params'])\n",
    "    def conv_dog(kp, x):\n",
    "        kp = jax.tree_util.keystr(kp)\n",
    "        if 'Conv' in kp:\n",
    "            x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "        return x\n",
    "    variables['params'] = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "    variables['batch_stats'] = jax.tree_map(lambda stats: stats.reshape((1, stats.shape[0], 1, 1)), variables['batch_stats'])\n",
    "    return variables\n",
    "        \n",
    "\n",
    "resolution = 3\n",
    "\n",
    "variables = initialize(resnet11, 42, x)\n",
    "variables = jax.tree_map(lambda x: jnp.tile(x, (resolution,)+(1,)*len(x.shape)), variables)\n",
    "loss_archive, logit_archive = train(variables, train_ds, lr=0.001, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_archive[0].shape)    # [(pmap=4, vmap=3)] * epochs=100 -> pmean: [(pmap=4, vmap=3)] * epochs=100\n",
    "print(loss_archive[-1].mean(axis=0))\n",
    "\n",
    "print(logit_archive[0].shape)    # [(pmap=4, vmap=3)] * epochs=100 -> pmean: [(pmap=4, vmap=3)] * epochs=100\n",
    "print(logit_archive[-1].mean(axis=0).argmax(axis=-1))\n",
    "print(logit_archive[0].mean(axis=0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(loss_archive[:100])), loss_archive[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxrunner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
