{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD0HhNaki7mX"
      },
      "source": [
        "# Running convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5SjVXQaigis"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.lax import conv, conv_general_dilated\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQLyIoUoi4HE"
      },
      "outputs": [],
      "source": [
        "H, W = 6, 6\n",
        "I, O = 1, 1\n",
        "window_strides = (1, 1)\n",
        "\n",
        "x = jnp.ones((10, H, W, I))       # NHWC\n",
        "kernel = jnp.ones((3, 3, I, O)) * 0.1    # HWIO\n",
        "\n",
        "x = jnp.transpose(x, [0, 3, 1, 2])  # NCHW\n",
        "kernel = jnp.transpose(kernel, [3, 2, 0, 1])    # OIHW\n",
        "\n",
        "output = conv(x, kernel, window_strides=window_strides, padding='VALID')  # NCHW\n",
        "# print(output.shape)\n",
        "\n",
        "output = jnp.transpose(output, [0, 2, 3, 1])  # NHWC\n",
        "# print(output.shape)\n",
        "\n",
        "# print(x[0].reshape((H, W)))\n",
        "# print(kernel[0])\n",
        "# print(output[0].reshape((H, W)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV111fCli_mH"
      },
      "source": [
        "# Gradient on conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr55DroMjGaV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "train_ds = tfds.load('mnist', split='train')\n",
        "test_ds = tfds.load('mnist', split='test')\n",
        "\n",
        "def data_normalize(ds):\n",
        "    return ds.map(lambda sample: {\n",
        "        'image': tf.cast(sample['image'], tf.float32) / 255.,\n",
        "        'label': sample['label']\n",
        "    })\n",
        "\n",
        "train_ds = data_normalize(train_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
        "test_ds = data_normalize(test_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
        "\n",
        "total_batch = train_ds.cardinality().numpy()\n",
        "total_tbatch = test_ds.cardinality().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3qU_HZrkqJV",
        "outputId": "8c8166b0-69ff-45c0-9bcd-95f9155623cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0 =======================================\n",
            "\tkernel1:  -0.2579745 0.24605475 -0.00069521513\n",
            "\tkernel2:  -0.1540713 0.15285209 0.0004105228\n",
            "\tfc:  -0.040572066 0.040740248 -4.7104633e-05\n",
            "\tgrad1:  -0.0021712063 0.0014822792 -0.00013415546\n",
            "\tgrad2:  -0.0022889 0.0011049211 -2.571689e-06\n",
            "\tgrad3:  -0.0043050298 0.0030392187 -2.1796608e-13\n",
            "\tloss:  2.3015804\n",
            "\n",
            "Epoch 1 =======================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[81], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     59\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 60\u001b[0m value, grad \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m kernel1 \u001b[38;5;241m=\u001b[39m kernel1 \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     63\u001b[0m kernel2 \u001b[38;5;241m=\u001b[39m kernel2 \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad[\u001b[38;5;241m1\u001b[39m]\n",
            "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\api.py:739\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    737\u001b[0m   _check_input_dtype_grad(holomorphic, allow_int, leaf)\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[1;32m--> 739\u001b[0m   ans, vjp_py \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    741\u001b[0m   ans, vjp_py, aux \u001b[38;5;241m=\u001b[39m _vjp(\n\u001b[0;32m    742\u001b[0m       f_partial, \u001b[38;5;241m*\u001b[39mdyn_args, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_axes\u001b[38;5;241m=\u001b[39mreduce_axes)\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\api.py:2229\u001b[0m, in \u001b[0;36m_vjp\u001b[1;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[0;32m   2228\u001b[0m   flat_fun, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun_nokwargs(fun, in_tree)\n\u001b[1;32m-> 2229\u001b[0m   out_primal, out_vjp \u001b[38;5;241m=\u001b[39m \u001b[43mad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m      \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2231\u001b[0m   out_tree \u001b[38;5;241m=\u001b[39m out_tree()\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\interpreters\\ad.py:142\u001b[0m, in \u001b[0;36mvjp\u001b[1;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(traceable, primals, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, reduce_axes\u001b[38;5;241m=\u001b[39m()):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[1;32m--> 142\u001b[0m     out_primals, pvals, jaxpr, consts \u001b[38;5;241m=\u001b[39m \u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     out_primals, pvals, jaxpr, consts, aux \u001b[38;5;241m=\u001b[39m linearize(traceable, \u001b[38;5;241m*\u001b[39mprimals, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\interpreters\\ad.py:131\u001b[0m, in \u001b[0;36mlinearize\u001b[1;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m _, in_tree \u001b[38;5;241m=\u001b[39m tree_flatten(((primals, primals), {}))\n\u001b[0;32m    130\u001b[0m jvpfun_flat, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun(jvpfun, in_tree)\n\u001b[1;32m--> 131\u001b[0m jaxpr, out_pvals, consts \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvpfun_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m out_primals_pvals, out_tangents_pvals \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree(), out_pvals)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(out_primal_pval\u001b[38;5;241m.\u001b[39mis_known() \u001b[38;5;28;01mfor\u001b[39;00m out_primal_pval \u001b[38;5;129;01min\u001b[39;00m out_primals_pvals)\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\profiler.py:336\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    335\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:774\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[1;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mnew_main(JaxprTrace, name_stack\u001b[38;5;241m=\u001b[39mcurrent_name_stack) \u001b[38;5;28;01mas\u001b[39;00m main:\n\u001b[0;32m    773\u001b[0m   fun \u001b[38;5;241m=\u001b[39m trace_to_subjaxpr_nounits(fun, main, instantiate)\n\u001b[1;32m--> 774\u001b[0m   jaxpr, (out_pvals, consts, env) \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    775\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\n\u001b[0;32m    776\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m main, fun, env\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\linear_util.py:191\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[0;32m    194\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[0;32m    197\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
            "Cell \u001b[1;32mIn[81], line 35\u001b[0m, in \u001b[0;36mloss\u001b[1;34m(kernels, x, y)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(kernels, x, y):\n\u001b[1;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     l \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy_with_integer_labels(jnp\u001b[38;5;241m.\u001b[39mclip(x, \u001b[38;5;241m1e-10\u001b[39m, \u001b[38;5;241m1.\u001b[39m), y)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l\u001b[38;5;241m.\u001b[39mmean()\n",
            "Cell \u001b[1;32mIn[81], line 24\u001b[0m, in \u001b[0;36mnet\u001b[1;34m(kernels, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# (100, 16, 28, 28) = NCHW\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtranspose(x, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\lax\\convolution.py:196\u001b[0m, in \u001b[0;36mconv\u001b[1;34m(lhs, rhs, window_strides, padding, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv\u001b[39m(lhs: Array, rhs: Array, window_strides: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    175\u001b[0m          padding: \u001b[38;5;28mstr\u001b[39m, precision: lax\u001b[38;5;241m.\u001b[39mPrecisionLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    176\u001b[0m          preferred_element_type: DTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    177\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience wrapper around `conv_general_dilated`.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    An array containing the convolution result.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_general_dilated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\lax\\convolution.py:161\u001b[0m, in \u001b[0;36mconv_general_dilated\u001b[1;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    155\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding argument to conv_general_dilated should be a string or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence of (low, high) pairs, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    158\u001b[0m preferred_element_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m preferred_element_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(np\u001b[38;5;241m.\u001b[39mdtype(preferred_element_type)))\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_general_dilated_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\core.py:444\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    442\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    443\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[1;32m--> 444\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\core.py:447\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 447\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\interpreters\\ad.py:318\u001b[0m, in \u001b[0;36mJVPTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    316\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDifferentiation rule for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprimitive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[1;32m--> 318\u001b[0m primal_out, tangent_out \u001b[38;5;241m=\u001b[39m jvp(primals_in, tangents_in, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mmultiple_results:\n\u001b[0;32m    320\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [JVPTracer(\u001b[38;5;28mself\u001b[39m, x, t) \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(primal_out, tangent_out)]\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\interpreters\\ad.py:534\u001b[0m, in \u001b[0;36mstandard_jvp\u001b[1;34m(jvprules, primitive, primals, tangents, **params)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstandard_jvp\u001b[39m(jvprules, primitive, primals, tangents, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m--> 534\u001b[0m   val_out \u001b[38;5;241m=\u001b[39m primitive\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39mprimals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    535\u001b[0m   tangents_out \u001b[38;5;241m=\u001b[39m [rule(t, \u001b[38;5;241m*\u001b[39mprimals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;28;01mfor\u001b[39;00m rule, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(jvprules, tangents)\n\u001b[0;32m    536\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Zero]\n\u001b[0;32m    537\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m val_out, functools\u001b[38;5;241m.\u001b[39mreduce(add_tangents, tangents_out, Zero\u001b[38;5;241m.\u001b[39mfrom_value(val_out))\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\core.py:444\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    442\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    443\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[1;32m--> 444\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\core.py:447\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 447\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\core.py:935\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[1;32m--> 935\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
            "File \u001b[1;32mc:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\jax\\_src\\dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[1;34m(prim, *args, **params)\u001b[0m\n\u001b[0;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import optax\n",
        "import flax.linen as nn\n",
        "\n",
        "\n",
        "\n",
        "def net(kernels, x):\n",
        "\n",
        "    kernel1 = kernels[0]\n",
        "    kernel2 = kernels[1]\n",
        "    fc = kernels[2]\n",
        "\n",
        "    strides = (1, 1)\n",
        "    padding = 'SAME'\n",
        "\n",
        "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
        "    kernel1 = jnp.transpose(kernel1, [3, 2, 0, 1])\n",
        "    kernel2 = jnp.transpose(kernel2, [3, 2, 0, 1])\n",
        "\n",
        "    x = conv(x, kernel1, window_strides=strides, padding=padding)\n",
        "    x = jax.nn.relu(x)\n",
        "    # (100, 16, 28, 28) = NCHW\n",
        "\n",
        "\n",
        "    x = conv(x, kernel2, window_strides=strides, padding=padding)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
        "\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), padding='SAME', strides=(2, 2))\n",
        "\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = jnp.dot(x, fc)\n",
        "    return jax.nn.softmax(x)\n",
        "\n",
        "def loss(kernels, x, y):\n",
        "    x = net(kernels, x)\n",
        "    l = optax.softmax_cross_entropy_with_integer_labels(jnp.clip(x, 1e-10, 1.), y)\n",
        "    return l.mean()\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "kernel1 = jax.nn.initializers.xavier_normal()(jax.random.key(42), (3, 3, 1, 16))\n",
        "kernel2 = jax.nn.initializers.xavier_normal()(jax.random.key(43), (3, 3, 16, 32))\n",
        "fc = jax.nn.initializers.xavier_normal()(jax.random.key(44), (32 * 14 * 14, 10))\n",
        "\n",
        "kernels = (kernel1, kernel2, fc)\n",
        "\n",
        "\n",
        "def update(kernels, bn, x, y):\n",
        "    l = loss(kernels, x, y)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    print(f\"\\nEpoch {i} =======================================\")\n",
        "    for batch in train_ds.as_numpy_iterator():\n",
        "        x = batch['image']\n",
        "        y = batch['label']\n",
        "        value, grad = jax.value_and_grad(loss)(kernels, x, y)\n",
        "\n",
        "        kernel1 = kernel1 - lr * grad[0]\n",
        "        kernel2 = kernel2 - lr * grad[1]\n",
        "        fc = fc - lr * grad[2]\n",
        "        kernels = (kernel1, kernel2, fc)\n",
        "\n",
        "    print(\"\\tkernel1: \", kernel1.min(), kernel1.max(), kernel1.mean())\n",
        "    print(\"\\tkernel2: \", kernel2.min(), kernel2.max(), kernel2.mean())\n",
        "    print(\"\\tfc: \", fc.min(), fc.max(), fc.mean())\n",
        "    print(\"\\tgrad1: \", grad[0].min(), grad[0].max(), grad[0].mean())\n",
        "    print(\"\\tgrad2: \", grad[1].min(), grad[1].max(), grad[1].mean())\n",
        "    print(\"\\tgrad3: \", grad[2].min(), grad[2].max(), grad[2].mean())\n",
        "    print(\"\\tloss: \", value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optax\n",
        "import flax.linen as nn\n",
        "\n",
        "def bn(x, gamma, beta, running_mean, running_var, train=True):\n",
        "    if train:\n",
        "        mean_x = jnp.mean(x, axis=0, keepdims=True)\n",
        "        var_x = jnp.mean((x - mean_x) ** 2, axis=0, keepdims=True)\n",
        "        \n",
        "\n",
        "\n",
        "def net(kernels, x):\n",
        "\n",
        "    kernel1 = kernels[0]\n",
        "    gamma1 = kernels[1]\n",
        "    beta1 = kernels[2]\n",
        "    kernel2 = kernels[3]\n",
        "    gamma2 = kernels[4]\n",
        "    beta2 = kernels[5]\n",
        "    fc = kernels[6]\n",
        "\n",
        "    strides = (1, 1)\n",
        "    padding = 'SAME'\n",
        "\n",
        "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
        "    kernel1 = jnp.transpose(kernel1, [3, 2, 0, 1])\n",
        "    kernel2 = jnp.transpose(kernel2, [3, 2, 0, 1])\n",
        "\n",
        "    x = conv(x, kernel1, window_strides=strides, padding=padding)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = conv(x, kernel2, window_strides=strides, padding=padding)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
        "\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), padding='SAME', strides=(2, 2))\n",
        "\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = jnp.dot(x, fc)\n",
        "    return jax.nn.softmax(x)\n",
        "\n",
        "def loss(kernels, x, y):\n",
        "    x = net(kernels, x)\n",
        "    l = optax.softmax_cross_entropy_with_integer_labels(jnp.clip(x, 1e-10, 1.), y)\n",
        "    return l.mean()\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "kernel1 = jax.nn.initializers.xavier_normal()(jax.random.key(42), (3, 3, 1, 16))\n",
        "gamma1 = jnp.ones((1, 16, 1, 1))\n",
        "beta1 = jnp.zeros((1, 16, 1, 1))\n",
        "kernel2 = jax.nn.initializers.xavier_normal()(jax.random.key(43), (3, 3, 16, 32))\n",
        "gamma2 = jnp.ones((1, 32, 1, 1))\n",
        "beta2 = jnp.zeros((1, 32, 1, 1))\n",
        "fc = jax.nn.initializers.xavier_normal()(jax.random.key(44), (32 * 14 * 14, 10))\n",
        "\n",
        "kernels = (kernel1, gamma1, beta1, kernel2, gamma2, beta2, fc)\n",
        "\n",
        "\n",
        "def update(kernels, x, y):\n",
        "    l = loss(kernels, x, y)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    print(f\"\\nEpoch {i} =======================================\")\n",
        "    for batch in train_ds.as_numpy_iterator():\n",
        "        x = batch['image']\n",
        "        y = batch['label']\n",
        "        value, grad = jax.value_and_grad(loss)(kernels, x, y)\n",
        "\n",
        "        kernel1 = kernel1 - lr * grad[0]\n",
        "        kernel2 = kernel2 - lr * grad[1]\n",
        "        fc = fc - lr * grad[2]\n",
        "        kernels = (kernel1, kernel2, fc)\n",
        "\n",
        "    print(\"\\tkernel1: \", kernel1.min(), kernel1.max(), kernel1.mean())\n",
        "    print(\"\\tkernel2: \", kernel2.min(), kernel2.max(), kernel2.mean())\n",
        "    print(\"\\tfc: \", fc.min(), fc.max(), fc.mean())\n",
        "    print(\"\\tgrad1: \", grad[0].min(), grad[0].max(), grad[0].mean())\n",
        "    print(\"\\tgrad2: \", grad[1].min(), grad[1].max(), grad[1].mean())\n",
        "    print(\"\\tgrad3: \", grad[2].min(), grad[2].max(), grad[2].mean())\n",
        "    print(\"\\tloss: \", value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "j-a8IztrxZsv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'params': {'gamma': Array([[[[1.]]]], dtype=float32),\n",
              "  'beta': Array([[[[0.]]]], dtype=float32)},\n",
              " 'batch_stats': {'moving_mean': Array([[[[0.]]]], dtype=float32),\n",
              "  'moving_var': Array([[[[1.]]]], dtype=float32)}}"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def batch_norm(X, deterministic, gamma, beta, moving_mean, moving_var, eps,\n",
        "               momentum):\n",
        "    # Use `deterministic` to determine whether the current mode is training\n",
        "    # mode or prediction mode\n",
        "    if deterministic:\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        # `linen.Module.variables` have a `value` attribute containing the array\n",
        "        X_hat = (X - moving_mean.value) / jnp.sqrt(moving_var.value + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(axis=0)\n",
        "            var = ((X - mean) ** 2).mean(axis=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of `X`, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
        "            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n",
        "        # In training mode, the current mean and variance are used\n",
        "        X_hat = (X - mean) / jnp.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean.value = momentum * moving_mean.value + (1.0 - momentum) * mean\n",
        "        moving_var.value = momentum * moving_var.value + (1.0 - momentum) * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    # `num_features`: the number of outputs for a fully connected layer\n",
        "    # or the number of output channels for a convolutional layer.\n",
        "    # `num_dims`: 2 for a fully connected layer and 4 for a convolutional layer\n",
        "    # Use `deterministic` to determine whether the current mode is training\n",
        "    # mode or prediction mode\n",
        "    num_features: int\n",
        "    num_dims: int\n",
        "    deterministic: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, X):\n",
        "        if self.num_dims == 2:\n",
        "            shape = (1, self.num_features)\n",
        "        else:\n",
        "            shape = (1, 1, 1, self.num_features)\n",
        "\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        gamma = self.param('gamma', jax.nn.initializers.ones, shape)\n",
        "        beta = self.param('beta', jax.nn.initializers.zeros, shape)\n",
        "\n",
        "        # The variables that are not model parameters are initialized to 0 and\n",
        "        # 1. Save them to the 'batch_stats' collection\n",
        "        moving_mean = self.variable('batch_stats', 'moving_mean', jnp.zeros, shape)\n",
        "        moving_var = self.variable('batch_stats', 'moving_var', jnp.ones, shape)\n",
        "        Y = batch_norm(X, self.deterministic, gamma, beta,\n",
        "                       moving_mean, moving_var, eps=1e-5, momentum=0.9)\n",
        "\n",
        "        return Y\n",
        "\n",
        "a = jnp.array([jnp.ones((28, 28, 1))*1, jnp.ones((28, 28, 1))*2, jnp.ones((28, 28, 1))*3, jnp.ones((28, 28, 1))*4, jnp.ones((28, 28, 1))*5])\n",
        "\n",
        "b = BatchNorm(1, 4, True)\n",
        "v = b.init(jax.random.key(42), a)\n",
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BatchNormLayer:\n",
        "\n",
        "    def __init__(self, dims: int) -> None:\n",
        "        self.gamma = np.ones((1, dims), dtype=\"float32\")\n",
        "        self.bias = np.zeros((1, dims), dtype=\"float32\")\n",
        "\n",
        "        self.running_mean_x = np.zeros(0)\n",
        "        self.running_var_x = np.zeros(0)\n",
        "\n",
        "        # forward params\n",
        "        self.var_x = np.zeros(0)\n",
        "        self.stddev_x = np.zeros(0)\n",
        "        self.x_minus_mean = np.zeros(0)\n",
        "        self.standard_x = np.zeros(0)\n",
        "        self.num_examples = 0\n",
        "        self.mean_x = np.zeros(0)\n",
        "        self.running_avg_gamma = 0.9\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "        # backward params\n",
        "        self.gamma_grad = np.zeros(0)\n",
        "        self.bias_grad = np.zeros(0)\n",
        "\n",
        "    def update_running_variables(self) -> None:\n",
        "        is_mean_empty = np.array_equal(np.zeros(0), self.running_mean_x)\n",
        "        is_var_empty = np.array_equal(np.zeros(0), self.running_var_x)\n",
        "        if is_mean_empty != is_var_empty:\n",
        "            raise ValueError(\"Mean and Var running averages should be \"\n",
        "                             \"initilizaded at the same time\")\n",
        "        if is_mean_empty:\n",
        "            self.running_mean_x = self.mean_x\n",
        "            self.running_var_x = self.var_x\n",
        "        else:\n",
        "            gamma = self.running_avg_gamma\n",
        "            self.running_mean_x = gamma * self.running_mean_x + \\\n",
        "                                  (1.0 - gamma) * self.mean_x\n",
        "            self.running_var_x = gamma * self.running_var_x + \\\n",
        "                                 (1. - gamma) * self.var_x\n",
        "\n",
        "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
        "        self.num_examples = x.shape[0]\n",
        "        if train:\n",
        "            self.mean_x = np.mean(x, axis=0, keepdims=True)\n",
        "            self.var_x = np.mean((x - self.mean_x) ** 2, axis=0, keepdims=True)\n",
        "            self.update_running_variables()\n",
        "        else:\n",
        "            self.mean_x = self.running_mean_x.copy()\n",
        "            self.var_x = self.running_var_x.copy()\n",
        "\n",
        "        self.var_x += self.epsilon\n",
        "        self.stddev_x = np.sqrt(self.var_x)\n",
        "        self.x_minus_mean = x - self.mean_x\n",
        "        self.standard_x = self.x_minus_mean / self.stddev_x\n",
        "        return self.gamma * self.standard_x + self.bias\n",
        "\n",
        "    def backward(self, grad_input: np.ndarray) -> np.ndarray:\n",
        "        standard_grad = grad_input * self.gamma\n",
        "\n",
        "        var_grad = np.sum(standard_grad * self.x_minus_mean * -0.5 * self.var_x ** (-3/2),\n",
        "                          axis=0, keepdims=True)\n",
        "        stddev_inv = 1 / self.stddev_x\n",
        "        aux_x_minus_mean = 2 * self.x_minus_mean / self.num_examples\n",
        "\n",
        "        mean_grad = (np.sum(standard_grad * -stddev_inv, axis=0,\n",
        "                            keepdims=True) +\n",
        "                            var_grad * np.sum(-aux_x_minus_mean, axis=0,\n",
        "                            keepdims=True))\n",
        "\n",
        "        self.gamma_grad = np.sum(grad_input * self.standard_x, axis=0,\n",
        "                                 keepdims=True)\n",
        "        self.bias_grad = np.sum(grad_input, axis=0, keepdims=True)\n",
        "\n",
        "        return standard_grad * stddev_inv + var_grad * aux_x_minus_mean + \\\n",
        "               mean_grad / self.num_examples\n",
        "\n",
        "    def apply_gradients(self, learning_rate: float) -> None:\n",
        "        self.gamma -= learning_rate * self.gamma_grad\n",
        "        self.bias -= learning_rate * self.bias_grad\n",
        "\n",
        "bn = BatchNormLayer(dims=4)\n",
        "\n",
        "o = bn.forward(x)\n",
        "bn.backward(np.ones_like(o))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BatchNormLayer:\n",
        "\n",
        "    def __init__(self, dims: int) -> None:\n",
        "        self.gamma = jnp.ones((1, dims), dtype=\"float32\")\n",
        "        self.bias = jnp.zeros((1, dims), dtype=\"float32\")\n",
        "\n",
        "        self.running_mean_x = jnp.zeros(0)\n",
        "        self.running_var_x = jnp.zeros(0)\n",
        "\n",
        "        # forward params\n",
        "        self.var_x = jnp.zeros(0)\n",
        "        self.stddev_x = jnp.zeros(0)\n",
        "        self.x_minus_mean = jnp.zeros(0)\n",
        "        self.standard_x = jnp.zeros(0)\n",
        "        self.num_examples = 0\n",
        "        self.mean_x = jnp.zeros(0)\n",
        "        self.running_avg_gamma = 0.9\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "        # backward params\n",
        "        self.gamma_grad = jnp.zeros(0)\n",
        "        self.bias_grad = jnp.zeros(0)\n",
        "\n",
        "    def update_running_variables(self) -> None:\n",
        "        is_mean_empty = jnp.array_equal(jnp.zeros(0), self.running_mean_x)\n",
        "        is_var_empty = jnp.array_equal(jnp.zeros(0), self.running_var_x)\n",
        "        if is_mean_empty != is_var_empty:\n",
        "            raise ValueError(\"Mean and Var running averages should be \"\n",
        "                             \"initilizaded at the same time\")\n",
        "        if is_mean_empty:\n",
        "            self.running_mean_x = self.mean_x\n",
        "            self.running_var_x = self.var_x\n",
        "        else:\n",
        "            gamma = self.running_avg_gamma\n",
        "            self.running_mean_x = gamma * self.running_mean_x + \\\n",
        "                                  (1.0 - gamma) * self.mean_x\n",
        "            self.running_var_x = gamma * self.running_var_x + \\\n",
        "                                 (1. - gamma) * self.var_x\n",
        "\n",
        "    def forward(self, x: jnp.ndarray, train: bool = True) -> jnp.ndarray:\n",
        "        self.num_examples = x.shape[0]\n",
        "        if train:\n",
        "            self.mean_x = jnp.mean(x, axis=0, keepdims=True)\n",
        "            self.var_x = jnp.mean((x - self.mean_x) ** 2, axis=0, keepdims=True)\n",
        "            self.update_running_variables()\n",
        "        else:\n",
        "            self.mean_x = self.running_mean_x.copy()\n",
        "            self.var_x = self.running_var_x.copy()\n",
        "\n",
        "        self.var_x += self.epsilon\n",
        "        self.stddev_x = jnp.sqrt(self.var_x)\n",
        "        self.x_minus_mean = x - self.mean_x\n",
        "        self.standard_x = self.x_minus_mean / self.stddev_x\n",
        "        return self.gamma * self.standard_x + self.bias\n",
        "\n",
        "    def backward(self, grad_input: jnp.ndarray) -> jnp.ndarray:\n",
        "        standard_grad = grad_input * self.gamma\n",
        "\n",
        "        var_grad = jnp.sum(standard_grad * self.x_minus_mean * -0.5 * self.var_x ** (-3/2),\n",
        "                          axis=0, keepdims=True)\n",
        "        stddev_inv = 1 / self.stddev_x\n",
        "        aux_x_minus_mean = 2 * self.x_minus_mean / self.num_examples\n",
        "\n",
        "        mean_grad = (jnp.sum(standard_grad * -stddev_inv, axis=0,\n",
        "                            keepdims=True) +\n",
        "                            var_grad * jnp.sum(-aux_x_minus_mean, axis=0,\n",
        "                            keepdims=True))\n",
        "\n",
        "        self.gamma_grad = jnp.sum(grad_input * self.standard_x, axis=0,\n",
        "                                 keepdims=True)\n",
        "        self.bias_grad = jnp.sum(grad_input, axis=0, keepdims=True)\n",
        "\n",
        "        return standard_grad * stddev_inv + var_grad * aux_x_minus_mean + \\\n",
        "               mean_grad / self.num_examples\n",
        "\n",
        "    def apply_gradients(self, learning_rate: float) -> None:\n",
        "        self.gamma -= learning_rate * self.gamma_grad\n",
        "        self.bias -= learning_rate * self.bias_grad\n",
        "\n",
        "bn = BatchNormLayer(dims=4)\n",
        "\n",
        "o = bn.forward(x)\n",
        "bn.backward(jnp.ones_like(o))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
