{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\konyang\\anaconda3\\envs\\ATTNtorch\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "train_ds = tfds.load('mnist', split='train')\n",
    "test_ds = tfds.load('mnist', split='test')\n",
    "\n",
    "def data_normalize(ds):\n",
    "    return ds.map(lambda sample: {\n",
    "        'image': tf.cast(sample['image'], tf.float32) / 255.,\n",
    "        'label': sample['label']\n",
    "    })\n",
    "\n",
    "train_ds = data_normalize(train_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
    "test_ds = data_normalize(test_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
    "\n",
    "total_batch = train_ds.cardinality().numpy()\n",
    "total_tbatch = test_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class BatchNormLayer:\n",
    "\n",
    "    def __init__(self, dims: int) -> None:\n",
    "        self.gamma = jnp.ones((1, dims), dtype=\"float32\")\n",
    "        self.bias = jnp.zeros((1, dims), dtype=\"float32\")\n",
    "\n",
    "        self.running_mean_x = jnp.zeros(0)\n",
    "        self.running_var_x = jnp.zeros(0)\n",
    "\n",
    "        # forward params\n",
    "        self.var_x = jnp.zeros(0)\n",
    "        self.stddev_x = jnp.zeros(0)\n",
    "        self.x_minus_mean = jnp.zeros(0)\n",
    "        self.standard_x = jnp.zeros(0)\n",
    "        self.num_examples = 0\n",
    "        self.mean_x = jnp.zeros(0)\n",
    "        self.running_avg_gamma = 0.9\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "        # backward params\n",
    "        self.gamma_grad = jnp.zeros(0)\n",
    "        self.bias_grad = jnp.zeros(0)\n",
    "\n",
    "    def update_running_variables(self) -> None:\n",
    "        is_mean_empty = jnp.array_equal(jnp.zeros(0), self.running_mean_x)\n",
    "        is_var_empty = jnp.array_equal(jnp.zeros(0), self.running_var_x)\n",
    "        if is_mean_empty != is_var_empty:\n",
    "            raise ValueError(\"Mean and Var running averages should be \"\n",
    "                             \"initilizaded at the same time\")\n",
    "        if is_mean_empty:\n",
    "            self.running_mean_x = self.mean_x\n",
    "            self.running_var_x = self.var_x\n",
    "        else:\n",
    "            gamma = self.running_avg_gamma\n",
    "            self.running_mean_x = gamma * self.running_mean_x + \\\n",
    "                                  (1.0 - gamma) * self.mean_x\n",
    "            self.running_var_x = gamma * self.running_var_x + \\\n",
    "                                 (1. - gamma) * self.var_x\n",
    "\n",
    "    def forward(self, x: jnp.ndarray, train: bool = True) -> jnp.ndarray:\n",
    "        self.num_examples = x.shape[0]\n",
    "        if train:\n",
    "            self.mean_x = jnp.mean(x, axis=0, keepdims=True)\n",
    "            self.var_x = jnp.mean((x - self.mean_x) ** 2, axis=0, keepdims=True)\n",
    "            self.update_running_variables()\n",
    "        else:\n",
    "            self.mean_x = self.running_mean_x.copy()\n",
    "            self.var_x = self.running_var_x.copy()\n",
    "\n",
    "        self.var_x += self.epsilon\n",
    "        self.stddev_x = jnp.sqrt(self.var_x)\n",
    "        self.x_minus_mean = x - self.mean_x\n",
    "        self.standard_x = self.x_minus_mean / self.stddev_x\n",
    "        return self.gamma * self.standard_x + self.bias\n",
    "\n",
    "    def backward(self, grad_input: jnp.ndarray) -> jnp.ndarray:\n",
    "        standard_grad = grad_input * self.gamma\n",
    "\n",
    "        var_grad = jnp.sum(standard_grad * self.x_minus_mean * -0.5 * self.var_x ** (-3/2),\n",
    "                          axis=0, keepdims=True)\n",
    "        stddev_inv = 1 / self.stddev_x\n",
    "        aux_x_minus_mean = 2 * self.x_minus_mean / self.num_examples\n",
    "\n",
    "        mean_grad = (jnp.sum(standard_grad * -stddev_inv, axis=0,\n",
    "                            keepdims=True) +\n",
    "                            var_grad * jnp.sum(-aux_x_minus_mean, axis=0,\n",
    "                            keepdims=True))\n",
    "\n",
    "        self.gamma_grad = jnp.sum(grad_input * self.standard_x, axis=0,\n",
    "                                 keepdims=True)\n",
    "        self.bias_grad = jnp.sum(grad_input, axis=0, keepdims=True)\n",
    "\n",
    "        return standard_grad * stddev_inv + var_grad * aux_x_minus_mean + \\\n",
    "               mean_grad / self.num_examples\n",
    "\n",
    "    def apply_gradients(self, learning_rate: float) -> None:\n",
    "        self.gamma -= learning_rate * self.gamma_grad\n",
    "        self.bias -= learning_rate * self.bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28, 28, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = BatchNormLayer(dims=4)\n",
    "\n",
    "for batch in train_ds.as_numpy_iterator():\n",
    "    x = batch['image']\n",
    "    y = batch['label']\n",
    "    break\n",
    "\n",
    "o = bn.forward(x)\n",
    "g = bn.backward(jnp.ones_like(o))\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_state(lr)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optax\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        x = nn.Conv(16, (3, 3), strides=2, padding='SAME')(x)\n",
    "        return x\n",
    "\n",
    "def create_state(lr):\n",
    "    convnet = CNN()\n",
    "    params = convnet.init(jax.random.key(42), x)['params']\n",
    "    tx = optax.sgd(lr)\n",
    "    return flax.training.train_state.TrainState.create(apply_fn=convnet.apply, params=params, tx=tx)\n",
    "\n",
    "vmapped = nn.vmap(create_state, in_axes=(None, 0), out_axes=0, variable_axes={'params': None})\n",
    "vmapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATTNtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
