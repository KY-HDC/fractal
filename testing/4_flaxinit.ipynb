{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "train_ds = tfds.load('mnist', split='train')\n",
    "test_ds = tfds.load('mnist', split='test')\n",
    "\n",
    "def data_normalize(ds):\n",
    "    return ds.map(lambda sample: {\n",
    "        'image': tf.cast(sample['image'], tf.float32) / 255.,\n",
    "        'label': sample['label']\n",
    "    })\n",
    "\n",
    "train_ds = data_normalize(train_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
    "test_ds = data_normalize(test_ds).shuffle(buffer_size=10, seed=42).batch(100).prefetch(1).take(1000)\n",
    "\n",
    "total_batch = train_ds.cardinality().numpy()\n",
    "total_tbatch = test_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.resnet_v2 import *\n",
    "\n",
    "for batch in train_ds.as_numpy_iterator():\n",
    "    x = batch['image']\n",
    "    y = batch['label']\n",
    "\n",
    "resnet = ResNet(num_classes=10, act_fn=nn.relu, block_class=ResNetBlock)\n",
    "variables = initialize(resnet, 42, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "resnet_kernel_init = jax.nn.initializers.variance_scaling(2.0, mode='fan_out', distribution='normal')\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    act_fn : callable  # Activation function\n",
    "    c_out : int   # Output feature size\n",
    "    subsample : bool = False  # If True, we apply a stride inside F\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # Network representing F\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    strides=(1, 1) if not self.subsample else (2, 2),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(x)\n",
    "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
    "\n",
    "        if self.subsample:\n",
    "            x = nn.Conv(self.c_out, kernel_size=(1, 1), strides=(2, 2), kernel_init=resnet_kernel_init, use_bias=False)(x)\n",
    "\n",
    "        x_out = self.act_fn(z + x)\n",
    "        return x_out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    num_classes : int\n",
    "    act_fn : callable\n",
    "    block_class : nn.Module\n",
    "    num_blocks : tuple = (3, 3, 3)\n",
    "    c_hidden : tuple = (16, 32, 64)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # A first convolution on the original image to scale up the channel size\n",
    "        x = nn.Conv(self.c_hidden[0], kernel_size=(3, 3), kernel_init=resnet_kernel_init, use_bias=False)(x)\n",
    "        if self.block_class == ResNetBlock:  # If pre-activation block, we do not apply non-linearities yet\n",
    "            x = nn.BatchNorm()(x, use_running_average=not train)\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        # Creating the ResNet blocks\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            for bc in range(block_count):\n",
    "                # Subsample the first block of each group, except the very first one.\n",
    "                subsample = (bc == 0 and block_idx > 0)\n",
    "                # ResNet block\n",
    "                x = self.block_class(c_out=self.c_hidden[block_idx],\n",
    "                                     act_fn=self.act_fn,\n",
    "                                     subsample=subsample)(x, train=train)\n",
    "\n",
    "        # Mapping to classification output\n",
    "        x = x.mean(axis=(1, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(self.num_classes, use_bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.as_numpy_iterator():\n",
    "    x = batch['image']\n",
    "    y = batch['label']\n",
    "    break\n",
    "\n",
    "resnet11 = ResNet(num_classes=10, act_fn=nn.relu, block_class=ResNetBlock)\n",
    "print(nn.tabulate(resnet11, jax.random.PRNGKey(42))(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "variables = resnet11.init(jax.random.PRNGKey(42), x)\n",
    "pprint(jax.tree_map(jnp.shape, variables), width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "resolution = 256\n",
    "\n",
    "tiled_variables = jax.tree_map(lambda x: jnp.tile(x, (resolution,)+(1,)*len(x.shape)), variables)\n",
    "jax.tree_map(jnp.shape, tiled_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "@partial(jax.vmap, in_axes=0, out_axes=0)\n",
    "def f_dict(x):\n",
    "    return jax.tree_map(lambda x: x*2, x)\n",
    "\n",
    "res = 10\n",
    "\n",
    "xdict = {\n",
    "    'a': jnp.ones((res, 3, 3)),\n",
    "    'b': jnp.ones((res, 3, 5, 6)),\n",
    "    'c': jnp.ones((res,))\n",
    "}\n",
    "\n",
    "f_dict(xdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f_dict2(x):\n",
    "    for k, v in x.items():\n",
    "        if k == 'a':\n",
    "            x[k] = v * 0.01\n",
    "        if k == 'b':\n",
    "            x[k] = v * 2\n",
    "        if 'c' in k:\n",
    "            x[k] = v / 2\n",
    "    return x\n",
    "\n",
    "# print(f_dict2.lower(xdict).as_text('stablehlo'))\n",
    "with jax.checking_leaks():\n",
    "    txt = f_dict2.lower(xdict).compile().as_text()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(jax.tree_map(jnp.shape, variables)['params'])\n",
    "# jax.tree_map(jnp.shape, variables)['batch_stats']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* v1. resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "\n",
    "# @jax.jit\n",
    "def net_test(variables, x: jnp.array):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    # T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            # k_conv0 = jnp.transpose(v['Conv_0']['kernel'], [3, 2, 0, 1])\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=True)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            # k_conv1 = jnp.transpose(v['Conv_1']['kernel'], [3, 2, 0, 1])\n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=True)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                # k_conv2 = jnp.transpose(v['Conv_2']['kernel'], [3, 2, 0, 1])\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "    # running_mu = running_mu.reshape((1, running_mu.shape[0], 1, 1))\n",
    "    # running_var = running_var.reshape((1, running_var.shape[0], 1, 1))\n",
    "\n",
    "    if on_train == True:\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        running_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        x = (x - mu) / jnp.sqrt(var + eps)\n",
    "    else:\n",
    "        x = (x - running_mu) / jnp.sqrt(running_var + eps)\n",
    "    x = gamma * x + beta\n",
    "\n",
    "    # gamma = gamma.reshape((gamma.shape[1],))\n",
    "    # beta = beta.reshape((beta.shape[1],))\n",
    "    # running_mu = running_mu.reshape((running_mu.shape[1],))\n",
    "    # running_var = running_var.reshape((running_var.shape[1],))\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "    \n",
    "# def loss_fn(variables, x, y):\n",
    "#     logits, variables = net_test(variables, x)\n",
    "#     return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "\n",
    "# @jax.jit\n",
    "# def train_oneEpoch(variables, x, y, lr):\n",
    "#     (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)  \n",
    "#     return loss, variables, grads\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(variables, x, y, lr):\n",
    "    def loss_fn(variables, x, y):\n",
    "        logits, variables = net_test(variables, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "    \n",
    "    (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (loss, logits)\n",
    "    \n",
    "def train(variables, batches, lr, epochs):\n",
    "    loss_archive, logits_archive = [], []\n",
    "    lr = jax.tree_map(lambda x: jnp.array((lr), dtype=jnp.float32), variables['params'])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        for batch in batches.as_numpy_iterator():\n",
    "            x = batch['image']\n",
    "            y = batch['label']\n",
    "            variables, (loss, logits) = update_fn(variables, x, y, lr)\n",
    "        loss_archive.append(loss)\n",
    "        logits_archive.append(logits)\n",
    "    return loss_archive, logits_archive\n",
    "\n",
    "def initialize(module, rng, x):\n",
    "    variables = module.init(jax.random.PRNGKey(rng), x)\n",
    "    variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))  # 64 * 28**2\n",
    "    # variables['params'] = jax.tree_map(lambda param: jnp.transpose(param, (3, 2, 0, 1)), variables['params'])\n",
    "    def conv_dog(kp, x):\n",
    "        kp = jax.tree_util.keystr(kp)\n",
    "        if 'Conv' in kp:\n",
    "            x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "        return x\n",
    "    variables['params'] = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "    variables['batch_stats'] = jax.tree_map(lambda stats: stats.reshape((1, stats.shape[0], 1, 1)), variables['batch_stats'])\n",
    "    return variables\n",
    "        \n",
    "\n",
    "# variables = resnet11.init(jax.random.PRNGKey(42), x)\n",
    "\n",
    "# custom_variables = variables.copy()\n",
    "# custom_variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))\n",
    "# custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['mean'] = custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['mean'].reshape((1, custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['mean'].shape[0], 1, 1))\n",
    "# custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['var'] = custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['var'].reshape((1, custom_variables['batch_stats']['ResNetBlock_0']['BatchNorm_0']['var'].shape[0], 1, 1))\n",
    "variables = initialize(resnet11, 42, x)\n",
    "loss_archive, logits_archive = train(variables, train_ds, lr=0.01, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* v2. resnet+parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_data(data, n_devices):\n",
    "    data = data.reshape(n_devices, data.shape[0] // n_devices, *data.shape[1:])\n",
    "    return data\n",
    "\n",
    "shard_data(x, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def net_test(variables, x: jnp.array):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    \n",
    "    # input.T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=True)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=True)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "\n",
    "    if on_train == True:\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        running_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        x = (x - mu) / jnp.sqrt(var + eps)\n",
    "    else:\n",
    "        x = (x - running_mu) / jnp.sqrt(running_var + eps)\n",
    "    x = gamma * x + beta\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "    \n",
    "# @jax.jit\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(None, 0, 0, None), out_axes=(None, 0))\n",
    "def update_fn(variables, x, y, lr):\n",
    "    def loss_fn(variables, x, y):\n",
    "        logits, variables = net_test(variables, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "    \n",
    "    (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "    logits = jax.lax.pmean(logits, axis_name='batch')\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (loss, logits)\n",
    "\n",
    "def train(variables, batches, lr, epochs):\n",
    "    loss_archive, logits_archive = [], []\n",
    "    lr = jax.tree_map(lambda x: jnp.array((lr), dtype=jnp.float32), variables['params'])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        for batch in batches.as_numpy_iterator():\n",
    "            x = shard_data(batch['image'], 4)\n",
    "            y = shard_data(batch['label'], 4)\n",
    "            \n",
    "            variables, (loss, logits) = update_fn(variables, x, y, lr)\n",
    "        loss_archive.append(loss)\n",
    "        logits_archive.append(logits)\n",
    "    return loss_archive, logits_archive\n",
    "\n",
    "def initialize(module, rng, x):\n",
    "    variables = module.init(jax.random.PRNGKey(rng), x)\n",
    "    variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))  # 64 * 28**2\n",
    "    # variables['params'] = jax.tree_map(lambda param: jnp.transpose(param, (3, 2, 0, 1)), variables['params'])\n",
    "    def conv_dog(kp, x):\n",
    "        kp = jax.tree_util.keystr(kp)\n",
    "        if 'Conv' in kp:\n",
    "            x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "        return x\n",
    "    variables['params'] = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "    variables['batch_stats'] = jax.tree_map(lambda stats: stats.reshape((1, stats.shape[0], 1, 1)), variables['batch_stats'])\n",
    "    return variables\n",
    "        \n",
    "\n",
    "variables = initialize(resnet11, 42, x)\n",
    "loss_archive, logits_archive = train(variables, train_ds, lr=0.01, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def net_test(variables, x: jnp.array):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    \n",
    "    # input.T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=True)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=True)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "\n",
    "    if on_train == True:\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        running_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        x = (x - mu) / jnp.sqrt(var + eps)\n",
    "    else:\n",
    "        x = (x - running_mu) / jnp.sqrt(running_var + eps)\n",
    "    x = gamma * x + beta\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "    \n",
    "# @jax.jit\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(None, 0, 0, None), out_axes=(None, 0))\n",
    "def update_fn(variables, x, y, lr):\n",
    "    # @jax.jit\n",
    "    def loss_fn(variables, x, y):\n",
    "        logits, variables = net_test(variables, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "    \n",
    "    (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    # loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "    # logits = jax.lax.pmean(logits, axis_name='batch')\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (loss, logits)\n",
    "\n",
    "def train(variables, batches, lr, epochs):\n",
    "    loss_archive, logits_archive = [], []\n",
    "    lr = jax.tree_map(lambda x: jnp.array((lr), dtype=jnp.float32), variables['params'])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        for batch in batches.as_numpy_iterator():\n",
    "            x = shard_data(batch['image'], 4)\n",
    "            y = shard_data(batch['label'], 4)\n",
    "            \n",
    "            variables, (loss, logits) = update_fn(variables, x, y, lr)\n",
    "        loss_archive.append(loss)\n",
    "        logits_archive.append(logits)\n",
    "    return loss_archive, logits_archive\n",
    "\n",
    "def initialize(module, rng, x):\n",
    "    variables = module.init(jax.random.PRNGKey(rng), x)\n",
    "    variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))  # 64 * 28**2\n",
    "    # variables['params'] = jax.tree_map(lambda param: jnp.transpose(param, (3, 2, 0, 1)), variables['params'])\n",
    "    def conv_dog(kp, x):\n",
    "        kp = jax.tree_util.keystr(kp)\n",
    "        if 'Conv' in kp:\n",
    "            x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "        return x\n",
    "    variables['params'] = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "    variables['batch_stats'] = jax.tree_map(lambda stats: stats.reshape((1, stats.shape[0], 1, 1)), variables['batch_stats'])\n",
    "    return variables\n",
    "        \n",
    "\n",
    "variables = initialize(resnet11, 42, x)\n",
    "loss_archive, logits_archive = train(variables, train_ds, lr=0.01, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* v3. parallelism + vmapped theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def net_test(variables, x: jnp.array):\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    \n",
    "    # input.T\n",
    "    x = jnp.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "    # 1st conv\n",
    "    x = jax.lax.conv(x, params['Conv_0']['kernel'], window_strides=(1, 1), padding='SAME')\n",
    "    x = nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "\n",
    "    # ResNetBlocks; conv0-conv1-skip\n",
    "    for k, v in params.items():\n",
    "        if 'ResNetBlock' in k:\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            k_conv0 = v['Conv_0']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv0, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_0'] = batchnorm(x, v['BatchNorm_0'], batch_stats[k]['BatchNorm_0'], on_train=True)\n",
    "            x = nn.relu(x)\n",
    "            \n",
    "            k_conv1 = v['Conv_1']['kernel']\n",
    "            x = jax.lax.conv(x, k_conv1, window_strides=(1, 1), padding='SAME')\n",
    "            x, batch_stats[k]['BatchNorm_1'] = batchnorm(x, v['BatchNorm_1'], batch_stats[k]['BatchNorm_1'], on_train=True)\n",
    "            \n",
    "            if 'Conv_2' in v.keys():\n",
    "                k_conv2 = v['Conv_2']['kernel']\n",
    "                residual = jax.lax.conv(residual, k_conv2, window_strides=(1, 1), padding='SAME')\n",
    "            x += residual\n",
    "            x = nn.relu(x)\n",
    "\n",
    "    # FC\n",
    "    x = nn.avg_pool(x, window_shape=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    x = jnp.transpose(x, [0, 2, 3, 1])\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = jnp.dot(x, params['Dense_0']['kernel'])\n",
    "\n",
    "    # batch_stats\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "\n",
    "    return nn.softmax(x), variables\n",
    "\n",
    "def batchnorm(x, params_bn, batch_stats_bn, momentum=0.9, eps=1e-6, on_train=True):\n",
    "    '''Batch normalizing\n",
    "        *Args\n",
    "            params: variables['params']['BatchNorm_X']\n",
    "            batch_stats: variables['batch_stats']['BatchNorm_X']\n",
    "    '''\n",
    "    gamma = params_bn['scale']\n",
    "    beta = params_bn['bias']\n",
    "    gamma = gamma.reshape((1, gamma.shape[0], 1, 1))\n",
    "    beta = beta.reshape((1, beta.shape[0], 1, 1))\n",
    "\n",
    "    running_mu = batch_stats_bn['mean']\n",
    "    running_var = batch_stats_bn['var']\n",
    "\n",
    "    if on_train == True:\n",
    "        mu = jnp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "        var = jnp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "        running_mu = momentum * running_mu + (1 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        x = (x - mu) / jnp.sqrt(var + eps)\n",
    "    else:\n",
    "        x = (x - running_mu) / jnp.sqrt(running_var + eps)\n",
    "    x = gamma * x + beta\n",
    "\n",
    "    batch_stats_bn['mean'] = running_mu\n",
    "    batch_stats_bn['var'] = running_var\n",
    "\n",
    "    return x, batch_stats_bn\n",
    "    \n",
    "# @jax.jit\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(None, 0, 0, None), out_axes=(None, 0))\n",
    "@partial(jax.vmap, in_axes=(0, None, None, None))\n",
    "def update_fn(variables, x, y, lr):\n",
    "    @jax.jit\n",
    "    def loss_fn(variables, x, y):\n",
    "        logits, variables = net_test(variables, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(jnp.clip(logits, 1e-6, 1.), y).mean(), (logits, variables)\n",
    "    \n",
    "    (loss, (logits, variables)), grads = jax.value_and_grad(loss_fn, has_aux=True)(variables, x, y)\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    # loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "    # logits = jax.lax.pmean(logits, axis_name='batch')\n",
    "    variables['params'] = jax.tree_map(lambda param, lr, g: param - lr * g, variables['params'], lr, grads['params'])\n",
    "    return variables, (loss, logits)\n",
    "\n",
    "def train(variables, batches, lr, epochs):\n",
    "    loss_archive, logits_archive = [], []\n",
    "    lr = jax.tree_map(lambda x: jnp.array((lr), dtype=jnp.float32), variables['params'])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        # vmapping needed\n",
    "        for batch in batches.as_numpy_iterator():\n",
    "            x = shard_data(batch['image'], 4)\n",
    "            y = shard_data(batch['label'], 4)\n",
    "            \n",
    "            variables, (loss, logits) = update_fn(variables, x, y, lr)\n",
    "        loss_archive.append(loss)\n",
    "        logits_archive.append(logits)\n",
    "    return loss_archive, logits_archive\n",
    "\n",
    "def initialize(module, rng, x):\n",
    "    variables = module.init(jax.random.PRNGKey(rng), x)\n",
    "    variables['params']['Dense_0']['kernel'] = jax.nn.initializers.xavier_normal()(jax.random.PRNGKey(1), (50176, 10))  # 64 * 28**2\n",
    "    # variables['params'] = jax.tree_map(lambda param: jnp.transpose(param, (3, 2, 0, 1)), variables['params'])\n",
    "    def conv_dog(kp, x):\n",
    "        kp = jax.tree_util.keystr(kp)\n",
    "        if 'Conv' in kp:\n",
    "            x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "        return x\n",
    "    variables['params'] = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "    variables['batch_stats'] = jax.tree_map(lambda stats: stats.reshape((1, stats.shape[0], 1, 1)), variables['batch_stats'])\n",
    "    return variables\n",
    "        \n",
    "\n",
    "resolution = 256\n",
    "\n",
    "variables = initialize(resnet11, 42, x)\n",
    "variables = jax.tree_map(lambda x: jnp.tile(x, (resolution,)+(1,)*len(x.shape)), variables)\n",
    "# loss_archive, logits_archive = train(variables, train_ds, lr=0.01, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = initialize(resnet11, 42, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.tree_map(lambda x, y, z: x * y + z, \n",
    "#              {'a': jnp.array([1,2,3]), 'b': jnp.array([4,5,6])}, \n",
    "#              {'a': jnp.array([0.01]), 'b': jnp.array([0.01])}, \n",
    "#              {'a': jnp.array([1,2,3]), 'b': jnp.array([4,5,6])})\n",
    "\n",
    "jax.tree_map(lambda x: jnp.array((0.01)), variables['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened, _ = jax.tree_util.tree_flatten_with_path(variables['params'])\n",
    "# for kp, v in flattened:\n",
    "#     print(jax.tree_util.keystr(kp))\n",
    "    # for k in kp:\n",
    "    #     k = k.key\n",
    "    #     if 'Conv' in k:\n",
    "    #         # print(jax.tree_util.tree_map_with_path(lambda kp, x: x.shape, variables['params']))\n",
    "    #         print(k)\n",
    "\n",
    "\n",
    "def conv_dog(kp, x):\n",
    "    kp = jax.tree_util.keystr(kp)\n",
    "    if 'Conv' in kp:\n",
    "        x = jnp.transpose(x, (3, 2, 0, 1))\n",
    "    return x\n",
    "\n",
    "vv = jax.tree_util.tree_map_with_path(conv_dog, variables['params'])\n",
    "vv['Conv_0']['kernel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "def net(x: jnp.array, variables: dict, training=True) -> jnp.array:\n",
    "    blocks = variables['params']\n",
    "    for block in blocks:\n",
    "        conv = jax.lax.conv()\n",
    "        bn = batchnorm(..., training=True)    # memory batch_stats in this line\n",
    "        dot = jnp.dot()\n",
    "        relu = jax.nn.relu()\n",
    "    softmax = jax.nn.softmax()\n",
    "    return softmax\n",
    "\n",
    "def batchnorm(...):\n",
    "    bs = variables['batch_stats']\n",
    "    ...\n",
    "\n",
    "def loss_fn(variables: dict, x: jnp.array, y: jnp.array, training=True) -> jnp.array:\n",
    "    x = net(..., training=training)\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(x, y)\n",
    "\n",
    "@jax.jit\n",
    "@jax.vmap(in_axes=(0, 0, None, None))\n",
    "def train_oneEpoch(variables: dict, lr: jnp.array, x: jnp.array, y: jnp.array):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(...)\n",
    "            te(variables, grads, lr):\n",
    "        variables['params'] = variables['params'] - lr * grads  # jax.tree_map\n",
    "        return variables\n",
    "    trace_on_loss(loss)\n",
    "    variables = update(variables, grads, lr)\n",
    "    return variables\n",
    "\n",
    "@jax.jit\n",
    "@jax.vmap\n",
    "def validate_oneEpoch(variables, x, y):\n",
    "    loss = loss_fn(..., training=False)\n",
    "    trace_on_loss(loss)\n",
    "    \n",
    "def train(num_epochs, variables, lr, x, y):\n",
    "    for i in range(num_epochs):\n",
    "        variables = train_oneEpoch(...)\n",
    "        _ = validate_oneEpoch(...)\n",
    "\n",
    "    deque_fn()  # ???: class?? or return something??\n",
    "    return ...\n",
    "\n",
    "\n",
    "\n",
    "def trace_on_loss(loss):\n",
    "    # deques of loss and accuracy\n",
    "    ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATTNtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
